{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 121487,
     "status": "ok",
     "timestamp": 1581240320563,
     "user": {
      "displayName": "نیره سادات خلدی نسب",
      "photoUrl": "",
      "userId": "03153259151600659487"
     },
     "user_tz": -210
    },
    "id": "tnxXKDjq3jEL",
    "outputId": "99bcc3a8-4957-420c-f310-849de8e103df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 2.x selected.\n",
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "os.chdir('/content/drive/My Drive/Colab Notebooks/Machine_Translation/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5140,
     "status": "ok",
     "timestamp": 1581240329398,
     "user": {
      "displayName": "نیره سادات خلدی نسب",
      "photoUrl": "",
      "userId": "03153259151600659487"
     },
     "user_tz": -210
    },
    "id": "CoHQ-RmRWaBt",
    "outputId": "6af870ca-d88a-4915-8efa-e35b59f9ca4b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hallo!'"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('Text_data.txt') as f:\n",
    "    data = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "pair_data = np.array([np.array(x.strip().split(\"\\t\")) for x in data])[:,0:2]\n",
    "Eng = pair_data[:,0]\n",
    "Ger = pair_data[:,1]\n",
    "Ger[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rd0jw-eC3jEh"
   },
   "outputs": [],
   "source": [
    "# Converts the unicode file to ascii\n",
    "def unicode_to_ascii(s):\n",
    "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "      if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "  w = unicode_to_ascii(w.lower().strip())\n",
    "\n",
    "  # creating a space between a word and the punctuation following it\n",
    "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "  w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "  w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "\n",
    "  w = w.rstrip().strip()\n",
    "\n",
    "  # adding a start and an end token to the sentence\n",
    "  # so that the model know when to start and stop predicting.\n",
    "  w = '<start> ' + w + ' <end>'\n",
    "  return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OHn4Dct23jEm"
   },
   "outputs": [],
   "source": [
    "# 1. Remove the accents\n",
    "# 2. Clean the sentences\n",
    "# 3. Return word pairs in the format: [ENGLISH, SPANISH]\n",
    "def create_dataset(lang, num_examples):\n",
    "  Words = [preprocess_sentence(l) for l in lang[:num_examples]]\n",
    "\n",
    "  return Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 12647,
     "status": "ok",
     "timestamp": 1581244605554,
     "user": {
      "displayName": "نیره سادات خلدی نسب",
      "photoUrl": "",
      "userId": "03153259151600659487"
     },
     "user_tz": -210
    },
    "id": "cTbSbBz55QtF",
    "outputId": "b89a7517-dc72-4dcd-cf5d-bcf1999971f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> doubtless there exists in this world precisely the right woman for any given man to marry and vice versa but when you consider that a human being has the opportunity of being acquainted with only a few hundred people , and out of the few hundred that there are but a dozen or less whom he knows intimately , and out of the dozen , one or two friends at most , it will easily be seen , when we remember the number of millions who inhabit this world , that probably , since the earth was created , the right man has never yet met the right woman . <end>\n",
      "<start> ohne zweifel findet sich auf dieser welt zu jedem mann genau die richtige ehefrau und umgekehrt wenn man jedoch in betracht zieht , dass ein mensch nur gelegenheit hat , mit ein paar hundert anderen bekannt zu sein , von denen ihm nur ein dutzend oder weniger nahesteht , darunter hochstens ein oder zwei freunde , dann erahnt man eingedenk der millionen einwohner dieser welt leicht , dass seit erschaffung ebenderselben wohl noch nie der richtige mann der richtigen frau begegnet ist . <end>\n"
     ]
    }
   ],
   "source": [
    "English = tuple(create_dataset(pair_data[:,0], None))\n",
    "German  = tuple(create_dataset(pair_data[:,1], None))\n",
    "print(English[-1])\n",
    "print(German[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OmMZQpdO60dt"
   },
   "outputs": [],
   "source": [
    "def max_length(tensor):\n",
    "  return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bIOn8RCNDJXG"
   },
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "      filters='')\n",
    "  lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "\n",
    "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                         padding='post')\n",
    "\n",
    "  return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eAY9k49G3jE_"
   },
   "outputs": [],
   "source": [
    "def load_dataset(data, num_examples=None):\n",
    "  # creating cleaned input, output pairs\n",
    "  inp_lang = create_dataset(data[:,0], num_examples)\n",
    "  targ_lang = create_dataset(data[:,1], num_examples)\n",
    "\n",
    "  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "\n",
    "  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GOi42V79Ydlr"
   },
   "source": [
    "### Limit the size of the dataset to experiment faster (optional)\n",
    "\n",
    "Training on the complete dataset of >100,000 sentences will take a long time. To train faster, we can limit the size of the dataset to 30,000 sentences (of course, translation quality degrades with less data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cnxC7q-j3jFD"
   },
   "outputs": [],
   "source": [
    "# Try experimenting with the size of that dataset\n",
    "num_examples = 30000\n",
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(pair_data, num_examples)\n",
    "\n",
    "# Calculate max_length of the target tensors\n",
    "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 717,
     "status": "ok",
     "timestamp": 1581240430622,
     "user": {
      "displayName": "نیره سادات خلدی نسب",
      "photoUrl": "",
      "userId": "03153259151600659487"
     },
     "user_tz": -210
    },
    "id": "kUHgjbbLA1JQ",
    "outputId": "0879349d-ce98-42a8-fc9f-8e0f1244d8ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 10\n"
     ]
    }
   ],
   "source": [
    "print(max_length_targ, max_length_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1957,
     "status": "ok",
     "timestamp": 1581240434446,
     "user": {
      "displayName": "نیره سادات خلدی نسب",
      "photoUrl": "",
      "userId": "03153259151600659487"
     },
     "user_tz": -210
    },
    "id": "4QILQkOs3jFG",
    "outputId": "0fe0827e-8d3b-426d-f6f4-646042d2cbcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000 24000 6000 6000\n"
     ]
    }
   ],
   "source": [
    "# Creating training and validation sets using an 80-20 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "# Show length\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lJPmLZGMeD5q"
   },
   "outputs": [],
   "source": [
    "def convert(lang, tensor):\n",
    "  for t in tensor:\n",
    "    if t!=0:\n",
    "      print (\"%s <----> %d\" % (lang.index_word[t], t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1601,
     "status": "ok",
     "timestamp": 1581240505552,
     "user": {
      "displayName": "نیره سادات خلدی نسب",
      "photoUrl": "",
      "userId": "03153259151600659487"
     },
     "user_tz": -210
    },
    "id": "VXukARTDd7MT",
    "outputId": "d8905214-67e8-4397-dec7-715bbf4995e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language; index to word mapping\n",
      "<start> <----> 1\n",
      "take <----> 72\n",
      "my <----> 25\n",
      "horse <----> 472\n",
      ". <----> 3\n",
      "<end> <----> 2\n",
      "\n",
      "Target Language; index to word mapping\n",
      "<start> <----> 1\n",
      "nimm <----> 187\n",
      "mein <----> 44\n",
      "pferd <----> 572\n",
      ". <----> 3\n",
      "<end> <----> 2\n"
     ]
    }
   ],
   "source": [
    "print (\"Input Language; index to word mapping\")\n",
    "convert(inp_lang, input_tensor_train[0])\n",
    "print ()\n",
    "print (\"Target Language; index to word mapping\")\n",
    "convert(targ_lang, target_tensor_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rgCLkfv5uO3d"
   },
   "source": [
    "### Create a tf.data dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1471,
     "status": "ok",
     "timestamp": 1581240536464,
     "user": {
      "displayName": "نیره سادات خلدی نسب",
      "photoUrl": "",
      "userId": "03153259151600659487"
     },
     "user_tz": -210
    },
    "id": "TqHsArVZ3jFS",
    "outputId": "e048a3f3-881f-4349-f830-2feaf441e5f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "print(steps_per_epoch)\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1331,
     "status": "ok",
     "timestamp": 1581240539167,
     "user": {
      "displayName": "نیره سادات خلدی نسب",
      "photoUrl": "",
      "userId": "03153259151600659487"
     },
     "user_tz": -210
    },
    "id": "kfJ7kLz9B1L9",
    "outputId": "c3eda631-81e5-4050-fef1-2da8632f02be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4559\n"
     ]
    }
   ],
   "source": [
    "print(vocab_inp_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1557,
     "status": "ok",
     "timestamp": 1581240541614,
     "user": {
      "displayName": "نیره سادات خلدی نسب",
      "photoUrl": "",
      "userId": "03153259151600659487"
     },
     "user_tz": -210
    },
    "id": "cifYR_zWUZON",
    "outputId": "6225c6d8-3ae1-414c-e34a-86b0a975c84c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<start>': 1, '<end>': 2, '.': 3, 'tom': 4, 'i': 5, '?': 6, 'you': 7, 'is': 8, 'it': 9, 's': 10, 'a': 11, 't': 12, 'we': 13, 'm': 14, 'that': 15, 'the': 16, 'he': 17, 'me': 18, 're': 19, 'to': 20, 'was': 21, 'do': 22, 'this': 23, 'can': 24, 'my': 25, 'are': 26, 'll': 27, 'have': 28, 'your': 29, 'don': 30, 'they': 31, 'did': 32, 'what': 33, 'she': 34, '!': 35, 'go': 36, 'here': 37, 'not': 38, 'be': 39, 'like': 40, 'in': 41, ',': 42, 'know': 43, 'let': 44, 'up': 45, 'who': 46, 'come': 47, 'get': 48, 'has': 49, 'on': 50, 'want': 51, 'how': 52, 'him': 53, 'isn': 54, 'no': 55, 'need': 56, 'now': 57, 'very': 58, 'where': 59, 'too': 60, 'for': 61, 'us': 62, 'one': 63, 'love': 64, 'help': 65, 'out': 66, 'at': 67, 'there': 68, 'just': 69, 'stop': 70, 'will': 71, 'take': 72, 'look': 73, 'home': 74, 'all': 75, 'got': 76, 'please': 77, 'good': 78, 'am': 79, 've': 80, 'why': 81, 'car': 82, 'mary': 83, 'see': 84, 'of': 85, 'won': 86, 'didn': 87, 'with': 88, 'may': 89, 'her': 90, 'so': 91, 'an': 92, 'saw': 93, 'give': 94, 'made': 95, 'were': 96, 'his': 97, 'work': 98, 'must': 99, 'try': 100, 'back': 101, 'likes': 102, 'book': 103, 'down': 104, 'looks': 105, 'busy': 106, 'again': 107, 'away': 108, 'keep': 109, 'dog': 110, 'wasn': 111, 'money': 112, 'lost': 113, 'had': 114, 'tell': 115, 'happy': 116, 'off': 117, 'say': 118, 'never': 119, 'call': 120, 'them': 121, 'hate': 122, 'does': 123, 'wait': 124, 'eat': 125, 'leave': 126, 'well': 127, 'still': 128, 'ok': 129, 'went': 130, 'alone': 131, 'came': 132, 'miss': 133, 'going': 134, 'right': 135, 'loves': 136, 'cold': 137, 'time': 138, 'about': 139, 'found': 140, 'stay': 141, 'read': 142, 'nobody': 143, 'everyone': 144, 'lot': 145, 'bad': 146, 'could': 147, 'seems': 148, 'd': 149, 'think': 150, 'ask': 151, 'talk': 152, 'nice': 153, 'new': 154, 'job': 155, 'big': 156, 'make': 157, 'man': 158, 'hurt': 159, 'late': 160, 'looked': 161, 'over': 162, 'watch': 163, 'feel': 164, 'should': 165, 'find': 166, 'gave': 167, 'said': 168, 'ready': 169, 'and': 170, 'our': 171, 'live': 172, 'coming': 173, 'left': 174, 'fast': 175, 'room': 176, 'aren': 177, 'open': 178, 'today': 179, 'knew': 180, 'drink': 181, 'play': 182, 'boston': 183, 'fine': 184, 'died': 185, 'wants': 186, 'put': 187, 'wrong': 188, 'more': 189, 'felt': 190, 'old': 191, 'tired': 192, 'mine': 193, 'really': 194, 'hot': 195, 'yours': 196, 'day': 197, 'ran': 198, 'way': 199, 'fun': 200, 'knows': 201, 'hungry': 202, 'turn': 203, 'some': 204, 'broke': 205, 'hope': 206, 'took': 207, 'great': 208, 'sick': 209, 'these': 210, 'hard': 211, 'angry': 212, 'when': 213, 'hurry': 214, 'kept': 215, 'from': 216, 'swim': 217, 'true': 218, 'show': 219, 'sing': 220, 'life': 221, 'boy': 222, 'called': 223, 'nothing': 224, 'house': 225, 'cry': 226, 'walk': 227, 'yet': 228, 'drunk': 229, 'much': 230, 'ate': 231, 'by': 232, 'bring': 233, 'told': 234, 'name': 235, 'run': 236, 'sure': 237, 'trust': 238, 'school': 239, 'doing': 240, 'everybody': 241, 'use': 242, 'dead': 243, 'crying': 244, 'would': 245, 'sit': 246, 'start': 247, 'better': 248, 'father': 249, 'door': 250, 'seemed': 251, 'lie': 252, 'done': 253, 'needs': 254, 'water': 255, 'hair': 256, 'long': 257, 'pay': 258, 'mean': 259, 'safe': 260, 'win': 261, 'follow': 262, 'mad': 263, 'early': 264, 'upset': 265, 'eyes': 266, 'bed': 267, 'easy': 268, 'quiet': 269, 'bit': 270, 'bought': 271, 'enjoy': 272, 'only': 273, 'cat': 274, 'heard': 275, 'coffee': 276, 'already': 277, 'met': 278, 'yourself': 279, 'hat': 280, 'sleep': 281, 'french': 282, 'rich': 283, 'tv': 284, 'crazy': 285, 'hates': 286, 'red': 287, 'enough': 288, 'owe': 289, 'stupid': 290, 'quite': 291, 'bus': 292, 'fish': 293, 'might': 294, 'doesn': 295, 'shot': 296, 'drive': 297, 'food': 298, 'idea': 299, 'first': 300, 'listen': 301, 'loved': 302, 'gone': 303, 'buy': 304, 'those': 305, 'began': 306, 'lying': 307, 'check': 308, 'hear': 309, 'beer': 310, 'outside': 311, 'hit': 312, 'free': 313, 'cook': 314, 'young': 315, 'everything': 316, 'tried': 317, 'die': 318, 'married': 319, 'watching': 320, 'stand': 321, 'soon': 322, 'inside': 323, 'almost': 324, 'whose': 325, 'cool': 326, 'sad': 327, 'hurts': 328, 'sorry': 329, 'running': 330, 'wife': 331, 'plan': 332, 'been': 333, 'tall': 334, 'works': 335, 'answer': 336, 'sat': 337, 'afraid': 338, 'lives': 339, 'bag': 340, 'best': 341, 'hand': 342, 'friend': 343, 'small': 344, 'believe': 345, 'fell': 346, 'hold': 347, 'move': 348, 'careful': 349, 'night': 350, 'kind': 351, 'missed': 352, 'break': 353, 'asleep': 354, 'both': 355, 'kids': 356, 'happened': 357, 'teacher': 358, 'became': 359, 'couldn': 360, 'fair': 361, 'agree': 362, 'kiss': 363, 'serious': 364, 'wine': 365, 'as': 366, 'seat': 367, 'nervous': 368, 'joke': 369, 'lunch': 370, 'funny': 371, 'quickly': 372, 'always': 373, 'black': 374, 'working': 375, 'sweet': 376, 'key': 377, 'change': 378, 'family': 379, 'perfect': 380, 'next': 381, 'speak': 382, 'quit': 383, 'touch': 384, 'dogs': 385, 'forget': 386, 'dark': 387, 'scared': 388, 'kidding': 389, 'seen': 390, 'something': 391, 'friends': 392, 'lied': 393, 'helped': 394, 'smiled': 395, 'smart': 396, 'close': 397, 'asked': 398, 'liked': 399, 'stopped': 400, 'rest': 401, 'after': 402, 'son': 403, 'stayed': 404, 'started': 405, 'changed': 406, 'wrote': 407, 'caught': 408, 'arrived': 409, 'which': 410, 'two': 411, 'keys': 412, 'laughed': 413, 'bored': 414, 'worked': 415, 'walked': 416, 'yes': 417, 'clean': 418, 'mind': 419, 'doctor': 420, 'singing': 421, 'short': 422, 'joking': 423, 'tomorrow': 424, 'wanted': 425, 'brother': 426, 'beat': 427, 'fat': 428, 'awake': 429, 'around': 430, 'study': 431, 'milk': 432, 'once': 433, 'hey': 434, 'lawyer': 435, 'word': 436, 'getting': 437, 'paid': 438, 'write': 439, 'stood': 440, 'tea': 441, 'remember': 442, 'eating': 443, 'white': 444, 'lucky': 445, 'reading': 446, 'dream': 447, 'someone': 448, 'guys': 449, 'fire': 450, 'hug': 451, 'spoke': 452, 'men': 453, 'cut': 454, 'smoke': 455, 'books': 456, 'worried': 457, 'throw': 458, 'cannot': 459, 'bike': 460, 'phone': 461, 'killed': 462, 'born': 463, 'thanks': 464, 'shy': 465, 'cute': 466, 'calm': 467, 'sign': 468, 'fired': 469, 'lonely': 470, 'cats': 471, 'horse': 472, 'himself': 473, 'cried': 474, 'brave': 475, 'ahead': 476, 'hands': 477, 'drank': 478, 'music': 479, 'happen': 480, 'feet': 481, 'dirty': 482, 'later': 483, 'truth': 484, 'girl': 485, 'sounds': 486, 'lazy': 487, 'bet': 488, 'alive': 489, 'anybody': 490, 'liar': 491, 'curious': 492, 'wish': 493, 'send': 494, 'expert': 495, 'dinner': 496, 'mom': 497, 'student': 498, 'worse': 499, 'seem': 500, 'betrayed': 501, 'turned': 502, 'box': 503, 'cheated': 504, 'care': 505, 'problem': 506, 'jealous': 507, 'simple': 508, 'meet': 509, 'people': 510, 'needed': 511, 'trusted': 512, 'order': 513, 'fault': 514, 'pretty': 515, 'mother': 516, 'wake': 517, 'warn': 518, 'dying': 519, 'slept': 520, 'anyone': 521, 'meat': 522, 'shoot': 523, 'sleepy': 524, 'begin': 525, 'count': 526, 'naive': 527, 'winning': 528, 'monday': 529, 'dance': 530, 'invited': 531, 'anything': 532, 'gun': 533, 'explain': 534, 'sleeping': 535, 'looking': 536, 'arm': 537, 'own': 538, 'news': 539, 'confused': 540, 'maybe': 541, 'shut': 542, 'real': 543, 'waited': 544, 'weak': 545, 'poor': 546, 'thirty': 547, 'failed': 548, 'even': 549, 'laugh': 550, 'blame': 551, 'snow': 552, 'face': 553, 'saved': 554, 'thirsty': 555, 'secret': 556, 'understand': 557, 'closed': 558, 'dad': 559, 'boss': 560, 'honest': 561, 'speaks': 562, 'wouldn': 563, 'fly': 564, 'blind': 565, 'far': 566, 'warm': 567, 'step': 568, 'duty': 569, 'slowly': 570, 'smell': 571, 'war': 572, 'hated': 573, 'age': 574, 'blue': 575, 'cake': 576, 'leaving': 577, 'girls': 578, 'pen': 579, 'learn': 580, 'rules': 581, 'ball': 582, 'fishing': 583, 'apple': 584, 'excited': 585, 'advice': 586, 'removed': 587, 'shoes': 588, 'little': 589, 'three': 590, 'ours': 591, 'patient': 592, 'boring': 593, 'pick': 594, 'strong': 595, 'talking': 596, 'broken': 597, 'kissed': 598, 'lose': 599, 'correct': 600, 'half': 601, 'snowing': 602, 'coat': 603, 'japanese': 604, 'says': 605, 'park': 606, 'truck': 607, 'guy': 608, 'trouble': 609, 'story': 610, 'full': 611, 'ugly': 612, 'catch': 613, 'tough': 614, 'drives': 615, 'often': 616, 'talks': 617, 'hugged': 618, 'map': 619, 'finished': 620, 'empty': 621, 'things': 622, 'woman': 623, 'pale': 624, 'or': 625, 'tennis': 626, 'deny': 627, 'somebody': 628, 'wear': 629, 'pleased': 630, 'party': 631, 'enemy': 632, 'badly': 633, 'smile': 634, 'hero': 635, 'relax': 636, 'forgot': 637, 'closer': 638, 'gas': 639, 'scream': 640, 'child': 641, 'pain': 642, 'last': 643, 'myself': 644, 'canadian': 645, 'guess': 646, 'guilty': 647, 'any': 648, 'horses': 649, 'helping': 650, 'warned': 651, 'missing': 652, 'six': 653, 'glasses': 654, 'heart': 655, 'swimming': 656, 'afford': 657, 'every': 658, 'movie': 659, 'knife': 660, 'wet': 661, 'save': 662, 'woke': 663, 'agreed': 664, 'talked': 665, 'choose': 666, 'along': 667, 'forgive': 668, 'clever': 669, 'famous': 670, 'contact': 671, 'fight': 672, 'useless': 673, 'rain': 674, 'pizza': 675, 'ears': 676, 'kill': 677, 'smells': 678, 'kid': 679, 'impressed': 680, 'surprised': 681, 'head': 682, 'brown': 683, 'normal': 684, 'second': 685, 'enjoyed': 686, 'light': 687, 'handle': 688, 'worth': 689, 'makes': 690, 'twice': 691, 'into': 692, 'children': 693, 'sister': 694, 'cooperating': 695, 'sounded': 696, 'voice': 697, 'sang': 698, 'runs': 699, 'welcome': 700, 'game': 701, 'ill': 702, 'thank': 703, 'drove': 704, 'god': 705, 'losing': 706, 'moving': 707, 'vote': 708, 'healthy': 709, 'humming': 710, 'law': 711, 'many': 712, 'shall': 713, 'drop': 714, 'studying': 715, 'raining': 716, 'strange': 717, 'smoking': 718, 'heavy': 719, 'sent': 720, 'scare': 721, 'join': 722, 'comes': 723, 'nose': 724, 'table': 725, 'behind': 726, 'tie': 727, 'trip': 728, 'bath': 729, 'being': 730, 'mistake': 731, 'driver': 732, 'hi': 733, 'hello': 734, 'fix': 735, 'promise': 736, 'obey': 737, 'hired': 738, 'cruel': 739, 'ice': 740, 'rude': 741, 'green': 742, 'bread': 743, 'wise': 744, 'forward': 745, 'naked': 746, 'jokes': 747, 'cab': 748, 'fool': 749, 'upstairs': 750, 'writing': 751, 'yelling': 752, 'meant': 753, 'dreaming': 754, 'annoying': 755, 'beautiful': 756, 'danger': 757, 'followed': 758, 'making': 759, 'overweight': 760, 'taking': 761, 'bill': 762, 'soccer': 763, 'message': 764, 'tonight': 765, 'yesterday': 766, 'week': 767, 'question': 768, 'goodbye': 769, 'okay': 770, 'thin': 771, 'weird': 772, 'jump': 773, 'walks': 774, 'moved': 775, 'voted': 776, 'carry': 777, 'built': 778, 'rice': 779, 'hiding': 780, 'sharp': 781, 'boys': 782, 'worry': 783, 'dizzy': 784, 'dancing': 785, 'retired': 786, 'wounded': 787, 'gets': 788, 'dumped': 789, 'apples': 790, 'sugar': 791, 'rarely': 792, 'divorced': 793, 'quietly': 794, 'set': 795, 'bird': 796, 'fed': 797, 'important': 798, 'saying': 799, 'moment': 800, 'cup': 801, 'baby': 802, 'skinny': 803, 'amazing': 804, 'polite': 805, 'escape': 806, 'sons': 807, 'point': 808, 'song': 809, 'believed': 810, 'homesick': 811, 'prisoner': 812, 'interested': 813, 'hour': 814, 'continue': 815, 'noise': 816, 'legs': 817, 'feeling': 818, 'decision': 819, 'pencil': 820, 'flag': 821, 'shirt': 822, 'sound': 823, 'helps': 824, 'picky': 825, 'burned': 826, 'survived': 827, 'ruined': 828, 'aside': 829, 'glad': 830, 'harder': 831, 'fit': 832, 'fruit': 833, 'eggs': 834, 'shocked': 835, 'unlucky': 836, 'wealthy': 837, 'bite': 838, 'stole': 839, 'boil': 840, 'egg': 841, 'pathetic': 842, 'ashamed': 843, 'proof': 844, 'sell': 845, 'sue': 846, 'coward': 847, 'grown': 848, 'else': 849, 'english': 850, 'type': 851, 'sells': 852, 'hide': 853, 'artist': 854, 'depressed': 855, 'different': 856, 'listening': 857, 'motivated': 858, 'nearby': 859, 'enter': 860, 'unfair': 861, 'foot': 862, 'cousins': 863, 'summer': 864, 'barely': 865, 'dressed': 866, 'wore': 867, 'wash': 868, 'students': 869, 'if': 870, 'terrible': 871, 'ordered': 872, 'asking': 873, 'window': 874, 'ever': 875, 'used': 876, 'choice': 877, 'part': 878, 'weren': 879, 'invite': 880, 'neighbor': 881, 'lock': 882, 'deep': 883, 'terrific': 884, 'birds': 885, 'envy': 886, 'panicked': 887, 'threw': 888, 'amused': 889, 'biased': 890, 'eaten': 891, 'town': 892, 'luck': 893, 'lips': 894, 'relaxed': 895, 'unusual': 896, 'trying': 897, 'blushed': 898, 'hers': 899, 'noticed': 900, 'refused': 901, 'fall': 902, 'deserve': 903, 'candy': 904, 'women': 905, 'thought': 906, 'genius': 907, 'innocent': 908, 'cancer': 909, 'walking': 910, 'control': 911, 'pass': 912, 'approve': 913, 'answered': 914, 'passed': 915, 'clock': 916, 'downstairs': 917, 'ignored': 918, 'boat': 919, 'orders': 920, 'sports': 921, 'mouse': 922, 'gate': 923, 'suits': 924, 'together': 925, 'laughing': 926, 'risk': 927, 'opened': 928, 'taught': 929, 'trusts': 930, 'anxious': 931, 'death': 932, 'also': 933, 'leg': 934, 'sisters': 935, 'animals': 936, 'having': 937, 'sense': 938, 'delicious': 939, 'their': 940, 'cooks': 941, 'picture': 942, 'train': 943, 'husband': 944, 'carefully': 945, 'same': 946, 'shopping': 947, 'haven': 948, 'taxi': 949, 'wrap': 950, 'believes': 951, 'past': 952, 'diary': 953, 'cap': 954, 'headache': 955, 'eye': 956, 'person': 957, 'storm': 958, 'breakfast': 959, 'tries': 960, 'refuse': 961, 'bald': 962, 'lies': 963, 'ignore': 964, 'yell': 965, 'nodded': 966, 'decide': 967, 'prepared': 968, 'math': 969, 'manage': 970, 'nurse': 971, 'unhappy': 972, 'list': 973, 'return': 974, 'thief': 975, 'escaped': 976, 'loud': 977, 'nuts': 978, 'deal': 979, 'teach': 980, 'another': 981, 'arrogant': 982, 'admire': 983, 'draw': 984, 'bleeding': 985, 'fighting': 986, 'dreams': 987, 'flowers': 988, 'few': 989, 'eats': 990, 'cars': 991, 'deserved': 992, 'forgave': 993, 'skiing': 994, 'nearly': 995, 'respect': 996, 'volunteered': 997, 'allow': 998, 'convinced': 999, 'impatient': 1000, 'uninsured': 1001, 'returned': 1002, 'possible': 1003, 'abroad': 1004, 'succeed': 1005, 'lay': 1006, 'ride': 1007, 'keeps': 1008, 'high': 1009, 'wide': 1010, 'spiders': 1011, 'untalented': 1012, 'sale': 1013, 'sore': 1014, 'plays': 1015, 'stabbed': 1016, 'brothers': 1017, 'studied': 1018, 'loudly': 1019, 'such': 1020, 'impossible': 1021, 'dress': 1022, 'bicycle': 1023, 'bank': 1024, 'courageous': 1025, 'tidy': 1026, 'taste': 1027, 'swam': 1028, 'fainted': 1029, 'wonderful': 1030, 'air': 1031, 'promised': 1032, 'greedy': 1033, 'replace': 1034, 'creative': 1035, 'wasted': 1036, 'cash': 1037, 'dieting': 1038, 'fake': 1039, 'mess': 1040, 'sushi': 1041, 'seldom': 1042, 'fail': 1043, 'driving': 1044, 'lead': 1045, 'closely': 1046, 'moody': 1047, 'insane': 1048, 'security': 1049, 'respond': 1050, 'paris': 1051, 'supper': 1052, 'furious': 1053, 'watched': 1054, 'satisfied': 1055, 'blood': 1056, 'takes': 1057, 'minute': 1058, 'dice': 1059, 'suit': 1060, 'doll': 1061, 'adores': 1062, 'applauded': 1063, 'raised': 1064, 'adopted': 1065, 'annoyed': 1066, 'cooking': 1067, 'engaged': 1068, 'robbed': 1069, 'paper': 1070, 'survive': 1071, 'modest': 1072, 'yoga': 1073, 'flew': 1074, 'beard': 1075, 'accept': 1076, 'hobby': 1077, 'revenge': 1078, 'rather': 1079, 'undressing': 1080, 'beyond': 1081, 'incorrect': 1082, 'apologize': 1083, 'tire': 1084, 'sings': 1085, 'foolish': 1086, 'mugged': 1087, 'waiting': 1088, 'owns': 1089, 'goes': 1090, 'harm': 1091, 'letter': 1092, 'owl': 1093, 'cheap': 1094, 'matter': 1095, 'sky': 1096, 'mouth': 1097, 'place': 1098, 'nonsense': 1099, 'desk': 1100, 'secrets': 1101, 'insulted': 1102, 'wasteful': 1103, 'lived': 1104, 'wears': 1105, 'brought': 1106, 'teeth': 1107, 'exam': 1108, 'world': 1109, 'comfortable': 1110, 'obstinate': 1111, 'finally': 1112, 'talkative': 1113, 'probably': 1114, 'expensive': 1115, 'thing': 1116, 'hang': 1117, 'push': 1118, 'jumped': 1119, 'excuse': 1120, 'fantastic': 1121, 'fear': 1122, 'snowed': 1123, 'course': 1124, 'then': 1125, 'waved': 1126, 'idiot': 1127, 'fill': 1128, 'coughed': 1129, 'lovely': 1130, 'canceled': 1131, 'fixed': 1132, 'resigned': 1133, 'happens': 1134, 'matters': 1135, 'clear': 1136, 'windy': 1137, 'cheese': 1138, 'sec': 1139, 'guts': 1140, 'fasting': 1141, 'debt': 1142, 'injured': 1143, 'popular': 1144, 'stunned': 1145, 'trapped': 1146, 'legal': 1147, 'rule': 1148, 'stinks': 1149, 'evil': 1150, 'listens': 1151, 'teaches': 1152, 'older': 1153, 'tipsy': 1154, 'armed': 1155, 'team': 1156, 'attentive': 1157, 'fbi': 1158, 'poet': 1159, 'horrible': 1160, 'smelled': 1161, 'singer': 1162, 'diabetic': 1163, 'angel': 1164, 'saint': 1165, 'steal': 1166, 'gold': 1167, 'scary': 1168, 'screamed': 1169, 'silent': 1170, 'years': 1171, 'reply': 1172, 'wins': 1173, 'rope': 1174, 'ring': 1175, 'resist': 1176, 'rely': 1177, 'rescued': 1178, 'prove': 1179, 'tourist': 1180, 'exhausted': 1181, 'flat': 1182, 'soup': 1183, 'number': 1184, 'copy': 1185, 'confessed': 1186, 'rushed': 1187, 'quick': 1188, 'violent': 1189, 'parents': 1190, 'towel': 1191, 'golf': 1192, 'year': 1193, 'end': 1194, 'fever': 1195, 'radio': 1196, 'sun': 1197, 'bananas': 1198, 'prefer': 1199, 'raise': 1200, 'illiterate': 1201, 'optimistic': 1202, 'using': 1203, 'smiling': 1204, 'weapon': 1205, 'confusing': 1206, 'awkward': 1207, 'line': 1208, 'test': 1209, 'hole': 1210, 'easily': 1211, 'grew': 1212, 'hurting': 1213, 'psychic': 1214, 'slapped': 1215, 'notes': 1216, 'bright': 1217, 'road': 1218, 'shook': 1219, 'side': 1220, 'whistle': 1221, 'cheating': 1222, 'surfing': 1223, 'lit': 1224, 'interesting': 1225, 'expect': 1226, 'relieved': 1227, 'dry': 1228, 'near': 1229, 'allowed': 1230, 'irrelevant': 1231, 'ridiculous': 1232, 'sea': 1233, 'fresh': 1234, 'blushing': 1235, 'arrested': 1236, 'drinking': 1237, 'attention': 1238, 'juice': 1239, 'yourselves': 1240, 'washed': 1241, 'pitch': 1242, 'forgetful': 1243, 'frightened': 1244, 'unreliable': 1245, 'incompetent': 1246, 'example': 1247, 'but': 1248, 'facebook': 1249, 'awesome': 1250, 'pull': 1251, 'dj': 1252, 'deaf': 1253, 'awful': 1254, 'ski': 1255, 'obese': 1256, 'stuck': 1257, 'cares': 1258, 'aboard': 1259, 'disagree': 1260, 'baking': 1261, 'chubby': 1262, 'humble': 1263, 'single': 1264, 'pray': 1265, 'oh': 1266, 'cheat': 1267, 'odd': 1268, 'phoned': 1269, 'yelled': 1270, 'wood': 1271, 'ruthless': 1272, 'relented': 1273, 'dozing': 1274, 'certain': 1275, 'chicken': 1276, 'selfish': 1277, 'staying': 1278, 'cloudy': 1279, 'locked': 1280, 'urgent': 1281, 'card': 1282, 'grow': 1283, 'drowned': 1284, 'vain': 1285, 'ship': 1286, 'objective': 1287, 'anyway': 1288, 'games': 1289, 'paint': 1290, 'lion': 1291, 'facts': 1292, 'bluffing': 1293, 'drowning': 1294, 'escaping': 1295, 'reliable': 1296, 'standing': 1297, 'starving': 1298, 'scares': 1299, 'shame': 1300, 'while': 1301, 'silly': 1302, 'witty': 1303, 'paying': 1304, 'add': 1305, 'questions': 1306, 'rush': 1307, 'kite': 1308, 'skating': 1309, 'tricked': 1310, 'american': 1311, 'blacked': 1312, 'led': 1313, 'movies': 1314, 'arabic': 1315, 'visited': 1316, 'concerned': 1317, 'damaged': 1318, 'trap': 1319, 'hopeless': 1320, 'stalling': 1321, 'peace': 1322, 'unkind': 1323, 'enemies': 1324, 'act': 1325, 'cows': 1326, 'notice': 1327, 'powerful': 1328, 'cookies': 1329, 'hunt': 1330, 'carrots': 1331, 'city': 1332, 'glass': 1333, 'popcorn': 1334, 'devastated': 1335, 'meditating': 1336, 'holiday': 1337, 'christmas': 1338, 'dangerous': 1339, 'difficult': 1340, 'prices': 1341, 'rubbish': 1342, 'price': 1343, 'chose': 1344, 'complained': 1345, 'present': 1346, 'charming': 1347, 'wind': 1348, 'lawn': 1349, 'camera': 1350, 'laughs': 1351, 'writes': 1352, 'candle': 1353, 'orange': 1354, 'breathe': 1355, 'skin': 1356, 'learned': 1357, 'potatoes': 1358, 'played': 1359, 'guitar': 1360, 'goal': 1361, 'notify': 1362, 'professor': 1363, 'color': 1364, 'enjoying': 1365, 'persevering': 1366, 'handsome': 1367, 'necessary': 1368, 'salty': 1369, 'blew': 1370, 'incredible': 1371, 'changes': 1372, 'repeat': 1373, 'attacked': 1374, 'baked': 1375, 'deserves': 1376, 'enjoys': 1377, 'company': 1378, 'smoker': 1379, 'fabulous': 1380, 'flexible': 1381, 'involved': 1382, 'bossy': 1383, 'weight': 1384, 'hilarious': 1385, 'cousin': 1386, 'than': 1387, 'concentrate': 1388, 'behaved': 1389, 'ratted': 1390, 'unconscious': 1391, 'wallet': 1392, 'video': 1393, 'drill': 1394, 'computer': 1395, 'stressed': 1396, 'belongs': 1397, 'drug': 1398, 'stomach': 1399, 'promoted': 1400, 'dropped': 1401, 'glanced': 1402, 'ambitious': 1403, 'contented': 1404, 'outspoken': 1405, 'others': 1406, 'alarm': 1407, 'weapons': 1408, 'days': 1409, 'appeared': 1410, 'address': 1411, 'hardly': 1412, 'clothes': 1413, 'daughter': 1414, 'breath': 1415, 'photo': 1416, 'downtown': 1417, 'tells': 1418, 'flight': 1419, 'tighten': 1420, 'fingers': 1421, 'shaved': 1422, 'giggled': 1423, 'bowed': 1424, 'human': 1425, 'chew': 1426, 'listened': 1427, 'pity': 1428, 'nap': 1429, 'agrees': 1430, 'burped': 1431, 'fought': 1432, 'smokes': 1433, 'snores': 1434, 'winked': 1435, 'discreet': 1436, 'comfort': 1437, 'panic': 1438, 'finish': 1439, 'jerk': 1440, 'class': 1441, 'dumb': 1442, 'bugs': 1443, 'jazz': 1444, 'rock': 1445, 'starved': 1446, 'slow': 1447, 'ten': 1448, 'safely': 1449, 'evening': 1450, 'courage': 1451, 'denied': 1452, 'kicked': 1453, 'baffled': 1454, 'piano': 1455, 'gullible': 1456, 'offended': 1457, 'positive': 1458, 'shooting': 1459, 'speaking': 1460, 'stubborn': 1461, 'thorough': 1462, 'salt': 1463, 'bedtime': 1464, 'spy': 1465, 'vanished': 1466, 'succeeded': 1467, 'attack': 1468, 'loser': 1469, 'reasonable': 1470, 'feed': 1471, 'coughing': 1472, 'belong': 1473, 'contributed': 1474, 'winter': 1475, 'sold': 1476, 'gift': 1477, 'neck': 1478, 'owls': 1479, 'tree': 1480, 'bell': 1481, 'absent': 1482, 'staggered': 1483, 'texted': 1484, 'packing': 1485, 'floor': 1486, 'doctors': 1487, 'special': 1488, 'mail': 1489, 'engine': 1490, 'rid': 1491, 'uncle': 1492, 'animal': 1493, 'disgusting': 1494, 'gifts': 1495, 'parties': 1496, 'dessert': 1497, 'puzzles': 1498, 'five': 1499, 'chair': 1500, 'captured': 1501, 'musician': 1502, 'overworked': 1503, 'successful': 1504, 'costs': 1505, 'pointless': 1506, 'large': 1507, 'doors': 1508, 'crime': 1509, 'annoys': 1510, 'theirs': 1511, 'blond': 1512, 'guard': 1513, 'playing': 1514, 'gentle': 1515, 'fearless': 1516, 'farm': 1517, 'helpless': 1518, 'times': 1519, 'content': 1520, 'stingy': 1521, 'sheep': 1522, 'bags': 1523, 'unmarried': 1524, 'stared': 1525, 'id': 1526, 'ticket': 1527, 'pens': 1528, 'courteous': 1529, 'mention': 1530, 'carpenter': 1531, 'lifeguard': 1532, 'policeman': 1533, 'able': 1534, 'calling': 1535, 'handed': 1536, 'seven': 1537, 'surprise': 1538, 'gray': 1539, 'stuff': 1540, 'moon': 1541, 'soap': 1542, 'each': 1543, 'uneasy': 1544, 'athletic': 1545, 'forgiven': 1546, 'shock': 1547, 'timid': 1548, 'cross': 1549, 'tense': 1550, 'available': 1551, 'confident': 1552, 'intrigued': 1553, 'prisoners': 1554, 'screaming': 1555, 'startled': 1556, 'mirror': 1557, 'deceive': 1558, 'sword': 1559, 'recently': 1560, 'aggressive': 1561, 'intelligent': 1562, 'sweat': 1563, 'powerless': 1564, 'hay': 1565, 'plumber': 1566, 'recognized': 1567, 'regret': 1568, 'humiliated': 1569, 'sunday': 1570, 'protect': 1571, 'avoiding': 1572, 'america': 1573, 'farmer': 1574, 'obvious': 1575, 'frozen': 1576, 'pulse': 1577, 'gain': 1578, 'windows': 1579, 'ended': 1580, 'fortunate': 1581, 'doubt': 1582, 'abandoned': 1583, 'corrected': 1584, 'prison': 1585, 'negligent': 1586, 'showed': 1587, 'nightmare': 1588, 'candles': 1589, 'hospital': 1590, 'lift': 1591, 'land': 1592, 'weather': 1593, 'finger': 1594, 'style': 1595, 'mood': 1596, 'roommate': 1597, 'familiar': 1598, 'rained': 1599, 'planned': 1600, 'greasy': 1601, 'attractive': 1602, 'meeting': 1603, 'taken': 1604, 'murderer': 1605, 'imaginative': 1606, 'wearing': 1607, 'promises': 1608, 'crow': 1609, 'wow': 1610, 'snore': 1611, 'brief': 1612, 'grab': 1613, 'humor': 1614, 'resign': 1615, 'aim': 1616, 'sober': 1617, 'sand': 1618, 'seriously': 1619, 'dozed': 1620, 'unlock': 1621, 'stink': 1622, 'faster': 1623, 'burns': 1624, 'admit': 1625, 'sneaky': 1626, 'cd': 1627, 'cover': 1628, 'cheats': 1629, 'drinks': 1630, 'obeyed': 1631, 'prayed': 1632, 'gives': 1633, 'merciful': 1634, 'sensible': 1635, 'stare': 1636, 'loaded': 1637, 'hesitated': 1638, 'recovered': 1639, 'ufo': 1640, 'cancel': 1641, 'resting': 1642, 'touched': 1643, 'poison': 1644, 'memorize': 1645, 'plants': 1646, 'release': 1647, 'action': 1648, 'blinked': 1649, 'cheered': 1650, 'painted': 1651, 'vomited': 1652, 'alert': 1653, 'bug': 1654, 'rifle': 1655, 'avoids': 1656, 'cranky': 1657, 'chinese': 1658, 'praying': 1659, 'improvised': 1660, 'addicted': 1661, 'eighteen': 1662, 'freezing': 1663, 'homeless': 1664, 'jam': 1665, 'managing': 1666, 'checked': 1667, 'immoral': 1668, 'pigs': 1669, 'jaw': 1670, 'fox': 1671, 'staring': 1672, 'approved': 1673, 'grumbled': 1674, 'frank': 1675, 'ex': 1676, 'vague': 1677, 'filthy': 1678, 'strict': 1679, 'dating': 1680, 'mistaken': 1681, 'favor': 1682, 'fry': 1683, 'stoned': 1684, 'grouch': 1685, 'friendly': 1686, 'tokyo': 1687, 'admired': 1688, 'chess': 1689, 'exaggerated': 1690, 'freaked': 1691, 'flu': 1692, 'salmon': 1693, 'trains': 1694, 'nature': 1695, 'pandas': 1696, 'stamps': 1697, 'violin': 1698, 'plane': 1699, 'pool': 1700, 'tempted': 1701, 'poems': 1702, 'easygoing': 1703, 'expecting': 1704, 'flattered': 1705, 'miserable': 1706, 'diet': 1707, 'skeptical': 1708, 'saturday': 1709, 'relief': 1710, 'improved': 1711, 'climbing': 1712, 'knee': 1713, 'roll': 1714, 'livid': 1715, 'shower': 1716, 'tastes': 1717, 'differ': 1718, 'fact': 1719, 'rang': 1720, 'japan': 1721, 'disagreed': 1722, 'plans': 1723, 'wicked': 1724, 'remembers': 1725, 'limping': 1726, 'lawyers': 1727, 'ain': 1728, 'object': 1729, 'tigers': 1730, 'complain': 1731, 'accelerated': 1732, 'disappeared': 1733, 'owes': 1734, 'borrow': 1735, 'church': 1736, 'stroke': 1737, 'jeans': 1738, 'insects': 1739, 'hurried': 1740, 'guessed': 1741, 'travel': 1742, 'taxes': 1743, 'poisoned': 1744, 'tortured': 1745, 'salesman': 1746, 'against': 1747, 'atheist': 1748, 'fascinated': 1749, 'oldest': 1750, 'insult': 1751, 'idiots': 1752, 'ditched': 1753, 'fooled': 1754, 'illegal': 1755, 'rose': 1756, 'roof': 1757, 'leaks': 1758, 'creepy': 1759, 'feels': 1760, 'grabbed': 1761, 'ford': 1762, 'intervened': 1763, 'miner': 1764, 'complex': 1765, 'dubious': 1766, 'focused': 1767, 'naughty': 1768, 'panting': 1769, 'puzzled': 1770, 'neat': 1771, 'pinched': 1772, 'risks': 1773, 'understood': 1774, 'cheerful': 1775, 'harmless': 1776, 'stranded': 1777, 'surrendered': 1778, 'decisive': 1779, 'famished': 1780, 'tip': 1781, 'honey': 1782, 'brush': 1783, 'rub': 1784, 'dawn': 1785, 'ambition': 1786, 'forty': 1787, 'adventurous': 1788, 'tongue': 1789, 'theory': 1790, 'baseball': 1791, 'football': 1792, 'misunderstood': 1793, 'bucket': 1794, 'scored': 1795, 'refund': 1796, 'bar': 1797, 'fork': 1798, 'assist': 1799, 'optimist': 1800, 'embarrassed': 1801, 'replaceable': 1802, 'texting': 1803, 'tied': 1804, 'breathing': 1805, 'river': 1806, 'acceptable': 1807, 'terrifying': 1808, 'served': 1809, 'mistakes': 1810, 'peel': 1811, 'remove': 1812, 'assertive': 1813, 'cards': 1814, 'aspirin': 1815, 'pills': 1816, 'russian': 1817, 'hotel': 1818, 'fate': 1819, 'psycho': 1820, 'generous': 1821, 'skittish': 1822, 'fussy': 1823, 'reads': 1824, 'alarmed': 1825, 'argue': 1826, 'nephew': 1827, 'earned': 1828, 'cooperate': 1829, 'giving': 1830, 'zoo': 1831, 'digging': 1832, 'unsociable': 1833, 'toy': 1834, 'france': 1835, 'gentleman': 1836, 'names': 1837, 'reach': 1838, 'considered': 1839, 'halloween': 1840, 'traveling': 1841, 'australia': 1842, 'beach': 1843, 'soul': 1844, 'partner': 1845, 'pants': 1846, 'vengeance': 1847, 'astonished': 1848, 'delighted': 1849, 'punished': 1850, 'translator': 1851, 'member': 1852, 'stopping': 1853, 'planet': 1854, 'seated': 1855, 'article': 1856, 'italy': 1857, 'studies': 1858, 'battery': 1859, 'rolls': 1860, 'jail': 1861, 'problems': 1862, 'headed': 1863, 'traitor': 1864, 'disturbed': 1865, 'inventive': 1866, 'shirtless': 1867, 'unsure': 1868, 'space': 1869, 'stepped': 1870, 'murdered': 1871, 'race': 1872, 'demand': 1873, 'depend': 1874, 'surrounded': 1875, 'hypocrite': 1876, 'exit': 1877, 'younger': 1878, 'mayor': 1879, 'beware': 1880, 'pet': 1881, 'earth': 1882, 'knees': 1883, 'honda': 1884, 'teaching': 1885, 'smiles': 1886, 'accident': 1887, 'store': 1888, 'before': 1889, 'monster': 1890, 'owner': 1891, 'client': 1892, 'pure': 1893, 'fits': 1894, 'depressing': 1895, 'impressive': 1896, 'straws': 1897, 'blank': 1898, 'shoulders': 1899, 'fashionable': 1900, 'contract': 1901, 'file': 1902, 'jammed': 1903, 'soldiers': 1904, 'burn': 1905, 'hasn': 1906, 'pig': 1907, 'threatened': 1908, 'coma': 1909, 'visitor': 1910, 'defenseless': 1911, 'weigh': 1912, 'arrive': 1913, 'responsible': 1914, 'patronize': 1915, 'helmet': 1916, 'forced': 1917, 'classmate': 1918, 'garden': 1919, 'tiger': 1920, 'judge': 1921, 'suitcase': 1922, 'airmail': 1923, 'stick': 1924, 'unambitious': 1925, 'volume': 1926, 'other': 1927, 'firm': 1928, 'nerve': 1929, 'yawned': 1930, 'marry': 1931, 'bless': 1932, 'clapped': 1933, 'hung': 1934, 'loosen': 1935, 'seize': 1936, 'froze': 1937, 'swims': 1938, 'higher': 1939, 'destroy': 1940, 'art': 1941, 'jesus': 1942, 'danced': 1943, 'dances': 1944, 'cds': 1945, 'specific': 1946, 'vigilant': 1947, 'flip': 1948, 'coin': 1949, 'eight': 1950, 'twin': 1951, 'online': 1952, 'mice': 1953, 'beef': 1954, 'overslept': 1955, 'surrender': 1956, 'klutz': 1957, 'vegan': 1958, 'finicky': 1959, 'sincere': 1960, 'unarmed': 1961, 'foggy': 1962, 'tasty': 1963, 'spring': 1964, 'means': 1965, 'slower': 1966, 'huge': 1967, 'decided': 1968, 'groaned': 1969, 'sneezed': 1970, 'sweated': 1971, 'pushy': 1972, 'share': 1973, 'cops': 1974, 'ghosts': 1975, 'driven': 1976, 'heroic': 1977, 'romantic': 1978, 'apologized': 1979, 'jogging': 1980, 'doubts': 1981, 'beans': 1982, 'liars': 1983, 'star': 1984, 'sympathize': 1985, 'shaken': 1986, 'artistic': 1987, 'expected': 1988, 'grateful': 1989, 'hungover': 1990, 'curse': 1991, 'low': 1992, 'softly': 1993, 'grimaced': 1994, 'objected': 1995, 'unbelievable': 1996, 'starve': 1997, 'amuse': 1998, 'fan': 1999, 'skate': 2000, 'faint': 2001, 'braces': 2002, 'autistic': 2003, 'muslim': 2004, 'sneezing': 2005, 'caused': 2006, 'dislike': 2007, 'onions': 2008, 'sweets': 2009, 'pony': 2010, 'page': 2011, 'painter': 2012, 'suspect': 2013, 'addict': 2014, 'orphan': 2015, 'p': 2016, 'gorgeous': 2017, 'treat': 2018, 'unlikely': 2019, 'unlocked': 2020, 'proceed': 2021, 'aloud': 2022, 'active': 2023, 'graduated': 2024, 'chef': 2025, 'crafty': 2026, 'joined': 2027, 'pushed': 2028, 'till': 2029, 'snoring': 2030, 'recycle': 2031, 'salad': 2032, 'flies': 2033, 'form': 2034, 'lines': 2035, 'writer': 2036, 'actor': 2037, 'outgoing': 2038, 'respects': 2039, 'scolded': 2040, 'fanatic': 2041, 'gambler': 2042, 'author': 2043, 'despised': 2044, 'tomatoes': 2045, 'hailed': 2046, 'spinach': 2047, 'ketchup': 2048, 'stories': 2049, 'turtles': 2050, 'college': 2051, 'stamp': 2052, 'answers': 2053, 'privacy': 2054, 'surgery': 2055, 'korean': 2056, 'usually': 2057, 'justice': 2058, 'sweating': 2059, 'aunt': 2060, 'beginner': 2061, 'officer': 2062, 'unemployed': 2063, 'remarried': 2064, 'hint': 2065, 'brand': 2066, 'kissing': 2067, 'lend': 2068, 'tag': 2069, 'power': 2070, 'pack': 2071, 'prepare': 2072, 'misses': 2073, 'pregnant': 2074, 'counting': 2075, 'gossiping': 2076, 'whistling': 2077, 'magic': 2078, 'logical': 2079, 'snag': 2080, 'appears': 2081, 'dug': 2082, 'roses': 2083, 'rats': 2084, 'falling': 2085, 'helpful': 2086, 'jittery': 2087, 'calmly': 2088, 'unhurt': 2089, 'sauce': 2090, 'guests': 2091, 'shots': 2092, 'country': 2093, 'envious': 2094, 'behave': 2095, 'skip': 2096, 'bother': 2097, 'misbehave': 2098, 'acts': 2099, 'dreamer': 2100, 'mentioned': 2101, 'shouts': 2102, 'outraged': 2103, 'photogenic': 2104, 'abhor': 2105, 'beg': 2106, 'contacted': 2107, 'cooked': 2108, 'downloaded': 2109, 'homework': 2110, 'rug': 2111, 'laptop': 2112, 'area': 2113, 'ladder': 2114, 'caffeine': 2115, 'boots': 2116, 'shave': 2117, 'relied': 2118, 'vitamins': 2119, 'horrified': 2120, 'mortified': 2121, 'defend': 2122, 'guest': 2123, 'niece': 2124, 'detective': 2125, 'begging': 2126, 'celebrating': 2127, 'daydreaming': 2128, 'arguing': 2129, 'vacation': 2130, 'poisonous': 2131, 'ago': 2132, 'sufficient': 2133, 'practicing': 2134, 'visit': 2135, 'seeing': 2136, 'october': 2137, 'bottle': 2138, 'beauty': 2139, 'straight': 2140, 'hitting': 2141, 'doubtful': 2142, 'soldier': 2143, 'yellow': 2144, 'acted': 2145, 'deceived': 2146, 'rabbits': 2147, 'drenched': 2148, 'ecstatic': 2149, 'insecure': 2150, 'unbiased': 2151, 'leaned': 2152, 'dazed': 2153, 'testify': 2154, 'crushed': 2155, 'budge': 2156, 'sank': 2157, 'brilliant': 2158, 'conscious': 2159, 'charge': 2160, 'lack': 2161, 'desperate': 2162, 'neighbors': 2163, 'plastered': 2164, 'stolen': 2165, 'comb': 2166, 'cop': 2167, 'barking': 2168, 'normally': 2169, 'tank': 2170, 'stairs': 2171, 'umbrella': 2172, 'muffins': 2173, 'shout': 2174, 'hunting': 2175, 'oranges': 2176, 'moves': 2177, 'acquitted': 2178, 'stark': 2179, 'violence': 2180, 'truly': 2181, 'drift': 2182, 'ripped': 2183, 'chemistry': 2184, 'cream': 2185, 'outsmarted': 2186, 'distracted': 2187, 'speechless': 2188, 'riser': 2189, 'ambidextrous': 2190, 'killing': 2191, 'overreacting': 2192, 'proud': 2193, 'located': 2194, 'delirious': 2195, 'washing': 2196, 'complicated': 2197, 'gotten': 2198, 'rainy': 2199, 'sticky': 2200, 'poker': 2201, 'lower': 2202, 'german': 2203, 'itchy': 2204, 'socks': 2205, 'perhaps': 2206, 'complaining': 2207, 'defeated': 2208, 'despises': 2209, 'size': 2210, 'snakes': 2211, 'scaring': 2212, 'showing': 2213, 'precautions': 2214, 'avoidable': 2215, 'ludicrous': 2216, 'twins': 2217, 'retire': 2218, 'wisely': 2219, 'trucker': 2220, 'competent': 2221, 'impulsive': 2222, 'obnoxious': 2223, 'perplexed': 2224, 'suspended': 2225, 'watches': 2226, 'hockey': 2227, 'bummed': 2228, 'released': 2229, 'dehydrated': 2230, 'hysterical': 2231, 'evidence': 2232, 'briefly': 2233, 'cheaper': 2234, 'uses': 2235, 'shouldn': 2236, 'under': 2237, 'error': 2238, 'ambulance': 2239, 'wanna': 2240, 'somewhere': 2241, 'goodnight': 2242, 'carried': 2243, 'hanged': 2244, 'trustworthy': 2245, 'sight': 2246, 'ballistic': 2247, 'bathe': 2248, 'passport': 2249, 'pink': 2250, 'recommended': 2251, 'jacket': 2252, 'scout': 2253, 'total': 2254, 'nine': 2255, 'reward': 2256, 'perfectly': 2257, 'inevitable': 2258, 'office': 2259, 'rising': 2260, 'shake': 2261, 'prediction': 2262, 'manners': 2263, 'toys': 2264, 'crashed': 2265, 'permit': 2266, 'bothering': 2267, 'wasting': 2268, 'dull': 2269, 'teachers': 2270, 'concerns': 2271, 'cookie': 2272, 'chickened': 2273, 'gained': 2274, 'prejudiced': 2275, 'leader': 2276, 'lacks': 2277, 'patience': 2278, 'remained': 2279, 'rented': 2280, 'consult': 2281, 'experienced': 2282, 'vodka': 2283, 'european': 2284, 'criminal': 2285, 'murder': 2286, 'mozart': 2287, 'robots': 2288, 'embarrass': 2289, 'blanks': 2290, 'sealed': 2291, 'confirm': 2292, 'cracked': 2293, 'boyfriend': 2294, 'comic': 2295, 'mathematics': 2296, 'month': 2297, 'success': 2298, 'squirrel': 2299, 'hoax': 2300, 'fridge': 2301, 'brass': 2302, 'cucumbers': 2303, 'schools': 2304, 'postcard': 2305, 'note': 2306, 'picked': 2307, 'slip': 2308, 'surprising': 2309, 'screws': 2310, 'screw': 2311, 'hunger': 2312, 'beekeeper': 2313, 'luggage': 2314, 'charismatic': 2315, 'extroverted': 2316, 'hardworking': 2317, 'introverted': 2318, 'turning': 2319, 'delete': 2320, 'detect': 2321, 'sarcasm': 2322, 'freeze': 2323, 'cuff': 2324, 'wept': 2325, 'bark': 2326, 'needy': 2327, 'tight': 2328, 'dig': 2329, 'peek': 2330, 'examine': 2331, 'gum': 2332, 'monk': 2333, 'rested': 2334, 'fad': 2335, 'bulky': 2336, 'mama': 2337, 'comment': 2338, 'gasped': 2339, 'moaned': 2340, 'sighed': 2341, 'smoked': 2342, 'tolerant': 2343, 'attend': 2344, 'baker': 2345, 'loner': 2346, 'medic': 2347, 'stuffed': 2348, 'through': 2349, 'yen': 2350, 'across': 2351, 'clap': 2352, 'exhaled': 2353, 'frowned': 2354, 'grinned': 2355, 'ocd': 2356, 'inhaled': 2357, 'kneeled': 2358, 'shouted': 2359, 'slipped': 2360, 'tripped': 2361, 'bore': 2362, 'loss': 2363, 'realistic': 2364, 'exist': 2365, 'easter': 2366, 'donut': 2367, 'snack': 2368, 'barbaric': 2369, 'dare': 2370, 'touching': 2371, 'lip': 2372, 'detest': 2373, 'wonder': 2374, 'hunter': 2375, 'purist': 2376, 'anorexic': 2377, 'dyslexic': 2378, 'grounded': 2379, 'rebel': 2380, 'restless': 2381, 'ticklish': 2382, 'iron': 2383, 'deer': 2384, 'begun': 2385, 'morning': 2386, 'ladies': 2387, 'cared': 2388, 'louder': 2389, 'filming': 2390, 'command': 2391, 'approves': 2392, 'chuckled': 2393, 'enlisted': 2394, 'flinched': 2395, 'insisted': 2396, 'shrugged': 2397, 'squinted': 2398, 'amazed': 2399, 'doomed': 2400, 'drown': 2401, 'lots': 2402, 'cough': 2403, 'clue': 2404, 'date': 2405, 'clocks': 2406, 'garlic': 2407, 'reggae': 2408, 'tulips': 2409, 'autumn': 2410, 'fist': 2411, 'support': 2412, 'twitter': 2413, 'nights': 2414, 'songs': 2415, 'redhead': 2416, 'gang': 2417, 'denial': 2418, 'llama': 2419, 'wig': 2420, 'apart': 2421, 'chilly': 2422, 'accurate': 2423, 'business': 2424, 'exciting': 2425, 'midnight': 2426, 'occupied': 2427, 'paddling': 2428, 'mortal': 2429, 'aches': 2430, 'bent': 2431, 'obeys': 2432, 'cutie': 2433, 'clearly': 2434, 'stir': 2435, 'bragging': 2436, 'shouting': 2437, 'dvd': 2438, 'exercised': 2439, 'frugal': 2440, 'loving': 2441, 'weary': 2442, 'west': 2443, 'zoomed': 2444, 'choking': 2445, 'elderly': 2446, 'tools': 2447, 'sinking': 2448, 'winners': 2449, 'responded': 2450, 'hiring': 2451, 'cage': 2452, 'grass': 2453, 'tempt': 2454, 'dislikes': 2455, 'delicate': 2456, 'learns': 2457, 'stern': 2458, 'con': 2459, 'outlaw': 2460, 'ached': 2461, 'brakes': 2462, 'hungarian': 2463, 'london': 2464, 'banana': 2465, 'gamble': 2466, 'mondays': 2467, 'amnesia': 2468, 'mahjong': 2469, 'seafood': 2470, 'sunsets': 2471, 'broom': 2472, 'hiccup': 2473, 'plead': 2474, 'sneeze': 2475, 'botanist': 2476, 'pacifist': 2477, 'teenager': 2478, 'weakling': 2479, 'quitter': 2480, 'bear': 2481, 'bargain': 2482, 'ambush': 2483, 'grotesque': 2484, 'risky': 2485, 'posted': 2486, 'practice': 2487, 'blonde': 2488, 'mum': 2489, 'ache': 2490, 'tooth': 2491, 'tune': 2492, 'science': 2493, 'seals': 2494, 'choked': 2495, 'teased': 2496, 'grumbling': 2497, 'bites': 2498, 'backed': 2499, 'bigger': 2500, 'grows': 2501, 'limp': 2502, 'biker': 2503, 'amusing': 2504, 'devious': 2505, 'devoted': 2506, 'enraged': 2507, 'excused': 2508, 'frantic': 2509, 'gasping': 2510, 'shaving': 2511, 'smashed': 2512, 'sobbing': 2513, 'spoiled': 2514, 'teasing': 2515, 'remembered': 2516, 'beaten': 2517, 'sewing': 2518, 'improve': 2519, 'grieving': 2520, 'grinning': 2521, 'unstable': 2522, 'gut': 2523, 'blast': 2524, 'earlier': 2525, 'partners': 2526, 'atm': 2527, 'quitting': 2528, 'wipe': 2529, 'morons': 2530, 'disgust': 2531, 'ghost': 2532, 'round': 2533, 'bees': 2534, 'police': 2535, 'congratulations': 2536, 'crows': 2537, 'confess': 2538, 'freak': 2539, 'interrupt': 2540, 'overdo': 2541, 'remind': 2542, 'tease': 2543, 'fetch': 2544, 'frogs': 2545, 'intrigues': 2546, 'frat': 2547, 'gardener': 2548, 'album': 2549, 'disobeyed': 2550, 'isolated': 2551, 'reptiles': 2552, 'weddings': 2553, 'freckles': 2554, 'showered': 2555, 'cartoons': 2556, 'misjudged': 2557, 'army': 2558, 'tissue': 2559, 'focus': 2560, 'giraffe': 2561, 'bacon': 2562, 'mommy': 2563, 'witness': 2564, 'dismissed': 2565, 'inform': 2566, 'rival': 2567, 'chauffeur': 2568, 'communist': 2569, 'housewife': 2570, 'therapist': 2571, 'engineer': 2572, 'stiff': 2573, 'spontaneous': 2574, 'unimpressed': 2575, 'babbling': 2576, 'friday': 2577, 'tasted': 2578, 'shortcut': 2579, 'metal': 2580, 'sour': 2581, 'negotiate': 2582, 'smaller': 2583, 'widow': 2584, 'merry': 2585, 'bets': 2586, 'drugs': 2587, 'bores': 2588, 'runner': 2589, 'graceful': 2590, 'seventeen': 2591, 'chances': 2592, 'frankly': 2593, 'cost': 2594, 'childish': 2595, 'personal': 2596, 'growled': 2597, 'stank': 2598, 'lake': 2599, 'leaves': 2600, 'melons': 2601, 'interns': 2602, 'admitted': 2603, 'pie': 2604, 'blamed': 2605, 'chased': 2606, 'designed': 2607, 'hiccups': 2608, 'inspired': 2609, 'hermit': 2610, 'maniac': 2611, 'priest': 2612, 'prince': 2613, 'racist': 2614, 'sailor': 2615, 'weirdo': 2616, 'adorable': 2617, 'agitated': 2618, 'barefoot': 2619, 'careless': 2620, 'clueless': 2621, 'dejected': 2622, 'disloyal': 2623, 'faithful': 2624, 'hesitant': 2625, 'impolite': 2626, 'insolent': 2627, 'kneeling': 2628, 'obedient': 2629, 'painting': 2630, 'punctual': 2631, 'reserved': 2632, 'studious': 2633, 'suicidal': 2634, 'talented': 2635, 'thirteen': 2636, 'thrilled': 2637, 'training': 2638, 'troubled': 2639, 'truthful': 2640, 'unafraid': 2641, 'pacing': 2642, 'pasta': 2643, 'novels': 2644, 'photos': 2645, 'understands': 2646, 'remain': 2647, 'suffer': 2648, 'gloves': 2649, 'charmer': 2650, 'suffering': 2651, 'thirtyish': 2652, 'walls': 2653, 'experts': 2654, 'camp': 2655, 'canadians': 2656, 'separated': 2657, 'prize': 2658, 'recover': 2659, 'less': 2660, 'catholic': 2661, 'bluff': 2662, 'consider': 2663, 'street': 2664, 'disturb': 2665, 'reason': 2666, 'noon': 2667, 'screams': 2668, 'soundly': 2669, 'historian': 2670, 'minded': 2671, 'widened': 2672, 'politely': 2673, 'comics': 2674, 'confronted': 2675, 'deliver': 2676, 'pizzas': 2677, 'reasons': 2678, 'computers': 2679, 'hypocrisy': 2680, 'receipt': 2681, 'heartburn': 2682, 'future': 2683, 'rights': 2684, 'chocolate': 2685, 'elephants': 2686, 'surprises': 2687, 'pounds': 2688, 'adventure': 2689, 'astronomy': 2690, 'gardening': 2691, 'woods': 2692, 'spaghetti': 2693, 'crowbar': 2694, 'aid': 2695, 'band': 2696, 'pointed': 2697, 'ink': 2698, 'frustrated': 2699, 'victorious': 2700, 'known': 2701, 'dishes': 2702, 'journalist': 2703, 'politician': 2704, 'shutterbug': 2705, 'disappointed': 2706, 'dissatisfied': 2707, 'housesitting': 2708, 'attic': 2709, 'lobby': 2710, 'inviting': 2711, 'riddle': 2712, 'suitable': 2713, 'flower': 2714, 'sunny': 2715, 'bitter': 2716, 'clearing': 2717, 'magnificent': 2718, 'unnecessary': 2719, 'repair': 2720, 'tiny': 2721, 'pump': 2722, 'tb': 2723, 'disliked': 2724, 'bmw': 2725, 'idolized': 2726, 'pianist': 2727, 'rejected': 2728, 'rode': 2729, 'camel': 2730, 'worships': 2731, 'swallow': 2732, 'affair': 2733, 'curtain': 2734, 'creaked': 2735, 'goat': 2736, 'lab': 2737, 'rough': 2738, 'appear': 2739, 'singers': 2740, 'critical': 2741, 'coach': 2742, 'case': 2743, 'avoided': 2744, 'created': 2745, 'dresses': 2746, 'forgives': 2747, 'immunity': 2748, 'olives': 2749, 'hummed': 2750, 'drummer': 2751, 'realtor': 2752, 'bilingual': 2753, 'committed': 2754, 'conceited': 2755, 'deceitful': 2756, 'demanding': 2757, 'disgusted': 2758, 'dishonest': 2759, 'efficient': 2760, 'emotional': 2761, 'excellent': 2762, 'impartial': 2763, 'improving': 2764, 'indignant': 2765, 'insincere': 2766, 'irritated': 2767, 'merciless': 2768, 'organized': 2769, 'overjoyed': 2770, 'sarcastic': 2771, 'sensitive': 2772, 'shivering': 2773, 'terrified': 2774, 'chief': 2775, 'trembling': 2776, 'waking': 2777, 'pushing': 2778, 'drowsy': 2779, 'wobbly': 2780, 'ought': 2781, 'bridge': 2782, 'settled': 2783, 'shuffled': 2784, 'suggested': 2785, 'wandered': 2786, 'cheering': 2787, 'cremated': 2788, 'deported': 2789, 'executed': 2790, 'unharmed': 2791, 'bonkers': 2792, 'trophy': 2793, 'delusional': 2794, 'headstrong': 2795, 'innovative': 2796, 'undefeated': 2797, 'historians': 2798, 'lifeguards': 2799, 'view': 2800, 'happening': 2801, 'lady': 2802, 'hamlet': 2803, 'nicely': 2804, 'quote': 2805, 'therapy': 2806, 'worthless': 2807, 'leaf': 2808, 'equal': 2809, 'arrest': 2810, 'deeply': 2811, 'court': 2812, 'adjourned': 2813, 'oversleep': 2814, 'grits': 2815, 'sip': 2816, 'spoon': 2817, 'blackmailed': 2818, 'blocked': 2819, 'cane': 2820, 'vase': 2821, 'living': 2822, 'wink': 2823, 'daredevil': 2824, 'physicist': 2825, 'scientist': 2826, 'influential': 2827, 'pigeon': 2828, 'toed': 2829, 'above': 2830, 'frugally': 2831, 'kyoto': 2832, 'obviously': 2833, 'miles': 2834, 'twisted': 2835, 'violated': 2836, 'rapidly': 2837, 'undergrad': 2838, 'georgia': 2839, 'hamburger': 2840, 'vw': 2841, 'suddenly': 2842, 'brushed': 2843, 'realize': 2844, 'interviews': 2845, 'backache': 2846, 'hangover': 2847, 'earache': 2848, 'sauerkraut': 2849, 'colors': 2850, 'advance': 2851, 'pulled': 2852, 'airplane': 2853, 'signed': 2854, 'swallowed': 2855, 'geography': 2856, 'otherwise': 2857, 'highway': 2858, 'sympathy': 2859, 'hours': 2860, 'firefighter': 2861, 'gym': 2862, 'wreck': 2863, 'flabbergasted': 2864, 'following': 2865, 'irreplaceable': 2866, 'guessing': 2867, 'cloud': 2868, 'curfew': 2869, 'fatal': 2870, 'depends': 2871, 'terribly': 2872, 'heavily': 2873, 'nonstop': 2874, 'cargo': 2875, 'emergency': 2876, 'starting': 2877, 'hustle': 2878, 'lions': 2879, 'stars': 2880, 'most': 2881, 'shoulder': 2882, 'volunteers': 2883, 'reconsider': 2884, 'masks': 2885, 'cursed': 2886, 'hyperactive': 2887, 'spin': 2888, 'webs': 2889, 'stealing': 2890, 'wild': 2891, 'medicine': 2892, 'essential': 2893, 'fir': 2894, 'crowd': 2895, 'lid': 2896, 'muddy': 2897, 'thick': 2898, 'tactic': 2899, 'worst': 2900, 'kidnapped': 2901, 'justified': 2902, 'borrowed': 2903, 'flinch': 2904, 'repulsed': 2905, 'speech': 2906, 'gladly': 2907, 'haircut': 2908, 'sunburned': 2909, 'bachelor': 2910, 'bookworm': 2911, 'diplomat': 2912, 'newcomer': 2913, 'despondent': 2914, 'distraught': 2915, 'distressed': 2916, 'former': 2917, 'holding': 2918, 'indiscreet': 2919, 'infallible': 2920, 'insightful': 2921, 'meticulous': 2922, 'nonplussed': 2923, 'outrageous': 2924, 'possessive': 2925, 'remarkable': 2926, 'unfriendly': 2927, 'hopeful': 2928, 'proposed': 2929, 'knocked': 2930, 'blondes': 2931, 'rent': 2932, 'shifted': 2933, 'gears': 2934, 'skipped': 2935, 'videotaped': 2936, 'payday': 2937, 'battle': 2938, 'plate': 2939, 'c': 2940, 'smashing': 2941, 'brewing': 2942, 'meals': 2943, 'included': 2944, 'buddhist': 2945, 'vegetarian': 2946, 'cause': 2947, 'stones': 2948, 'king': 2949, 'endorse': 2950, 'unique': 2951, 'exercise': 2952, 'banged': 2953, 'top': 2954, 'colleague': 2955, 'heartless': 2956, 'gossip': 2957, 'heartbroken': 2958, 'its': 2959, 'papers': 2960, 'economy': 2961, 'offer': 2962, 'imagine': 2963, 'exclude': 2964, 'translating': 2965, 'complaint': 2966, 'toothache': 2967, 'cameras': 2968, 'emailed': 2969, 'candlelight': 2970, 'disco': 2971, 'mowed': 2972, 'pad': 2973, 'envelope': 2974, 'lotion': 2975, 'butter': 2976, 'traveled': 2977, 'unplugged': 2978, 'learner': 2979, 'selling': 2980, 'mt': 2981, 'fuji': 2982, 'ignorance': 2983, 'bliss': 2984, 'christian': 2985, 'recyclable': 2986, 'purse': 2987, 'easier': 2988, 'mesmerizing': 2989, 'stuffy': 2990, 'typo': 2991, 'unforgettable': 2992, 'europe': 2993, 'jar': 2994, 'sack': 2995, 'luckily': 2996, 'final': 2997, 'project': 2998, 'teaser': 2999, 'somalia': 3000, 'godmother': 3001, 'correcting': 3002, 'tickles': 3003, 'mask': 3004, 'biggest': 3005, 'barn': 3006, 'jumping': 3007, 'caved': 3008, 'melting': 3009, 'brightened': 3010, 'urgency': 3011, 'words': 3012, 'ali': 3013, 'shoelaces': 3014, 'freed': 3015, 'hillbilly': 3016, 'womanizer': 3017, 'belligerent': 3018, 'recovery': 3019, 'charity': 3020, 'ostracized': 3021, 'mustn': 3022, 'tent': 3023, 'contributing': 3024, 'vile': 3025, 'behavior': 3026, 'discussed': 3027, 'visiting': 3028, 'lean': 3029, 'messed': 3030, 'fascinating': 3031, 'imminent': 3032, 'habits': 3033, 'meal': 3034, 'temple': 3035, 'disconnect': 3036, 'plug': 3037, 'exactly': 3038, 'cheers': 3039, 'hop': 3040, 'cheer': 3041, 'chill': 3042, 'cured': 3043, 'aah': 3044, 'below': 3045, 'knits': 3046, 'definitely': 3047, 'brag': 3048, 'calls': 3049, 'exists': 3050, 'faith': 3051, 'swiss': 3052, 'absurd': 3053, 'tragic': 3054, 'anemic': 3055, 'immune': 3056, 'mature': 3057, 'shiny': 3058, 'lasts': 3059, 'goofed': 3060, 'paints': 3061, 'paused': 3062, 'taller': 3063, 'assume': 3064, 'glue': 3065, 'oppose': 3066, 'futile': 3067, 'split': 3068, 'timing': 3069, 'curt': 3070, 'belched': 3071, 'gloated': 3072, 'slim': 3073, 'prepaid': 3074, 'flaky': 3075, 'cnn': 3076, 'hell': 3077, 'dump': 3078, 'ego': 3079, 'floats': 3080, 'abandon': 3081, 'describe': 3082, 'bigot': 3083, 'honesty': 3084, 'pays': 3085, 'caviar': 3086, 'burp': 3087, 'giddy': 3088, 'tests': 3089, 'hives': 3090, 'trips': 3091, 'suppose': 3092, 'pisces': 3093, 'perth': 3094, 'stronger': 3095, 'worn': 3096, 'solid': 3097, 'wolf': 3098, 'plant': 3099, 'bizarre': 3100, 'hailing': 3101, 'hearsay': 3102, 'hip': 3103, 'sued': 3104, 'babe': 3105, 'gawking': 3106, 'acne': 3107, 'vet': 3108, 'frail': 3109, 'loyal': 3110, 'messy': 3111, 'shivered': 3112, 'stutters': 3113, 'nosy': 3114, 'whistled': 3115, 'arabs': 3116, 'annoy': 3117, 'gross': 3118, 'mock': 3119, 'circle': 3120, 'healthily': 3121, 'peas': 3122, 'exhale': 3123, 'bloom': 3124, 'flying': 3125, 'ham': 3126, 'advises': 3127, 'blog': 3128, 'austrian': 3129, 'jesuit': 3130, 'bled': 3131, 'thrilling': 3132, 'despise': 3133, 'queasy': 3134, 'celery': 3135, 'visa': 3136, 'sinned': 3137, 'voices': 3138, 'donuts': 3139, 'spoons': 3140, 'camels': 3141, 'crew': 3142, 'loan': 3143, 'crown': 3144, 'rat': 3145, 'hiking': 3146, 'cripple': 3147, 'fireman': 3148, 'bat': 3149, 'sickens': 3150, 'superb': 3151, 'fungus': 3152, 'inhumane': 3153, 'obsolete': 3154, 'optional': 3155, 'shrieked': 3156, 'scarce': 3157, 'robe': 3158, 'ease': 3159, 'meddling': 3160, 'straighten': 3161, 'sweep': 3162, 'saturn': 3163, 'myth': 3164, 'doable': 3165, 'asian': 3166, 'fools': 3167, 'spies': 3168, 'pun': 3169, 'blames': 3170, 'exercises': 3171, 'jock': 3172, 'slob': 3173, 'temp': 3174, 'wimp': 3175, 'unwell': 3176, 'protested': 3177, 'wax': 3178, 'stalled': 3179, 'hassle': 3180, 'sweaty': 3181, 'coke': 3182, 'babies': 3183, 'qualified': 3184, 'boats': 3185, 'sink': 3186, 'brace': 3187, 'purr': 3188, 'dies': 3189, 'kick': 3190, 'afternoon': 3191, 'birthday': 3192, 'holidays': 3193, 'bankrupt': 3194, 'butcher': 3195, 'samurai': 3196, 'slacker': 3197, 'henpecked': 3198, 'perceptive': 3199, 'flunk': 3200, 'guarantee': 3201, 'ironing': 3202, 'karaoke': 3203, 'riddles': 3204, 'castles': 3205, 'drawing': 3206, 'noodles': 3207, 'oysters': 3208, 'sashimi': 3209, 'harvard': 3210, 'almonds': 3211, 'lasagna': 3212, 'ipod': 3213, 'decline': 3214, 'cattle': 3215, 'serve': 3216, 'puppy': 3217, 'outdoors': 3218, 'prosper': 3219, 'scold': 3220, 'farsighted': 3221, 'stew': 3222, 'mug': 3223, 'fee': 3224, 'wednesday': 3225, 'classic': 3226, 'admirable': 3227, 'forbidden': 3228, 'marvelous': 3229, 'redundant': 3230, 'searching': 3231, 'chest': 3232, 'joints': 3233, 'wrist': 3234, 'pout': 3235, 'gear': 3236, 'bach': 3237, 'types': 3238, 'hottie': 3239, 'snap': 3240, 'resisting': 3241, 'sniffling': 3242, 'ideal': 3243, 'given': 3244, 'skull': 3245, 'tower': 3246, 'evident': 3247, 'hogwash': 3248, 'plastic': 3249, 'typical': 3250, 'barked': 3251, 'adore': 3252, 'quarreled': 3253, 'struggled': 3254, 'approached': 3255, 'biked': 3256, 'dialed': 3257, 'draws': 3258, 'woozy': 3259, 'busted': 3260, 'fatter': 3261, 'hacked': 3262, 'ph': 3263, 'asthma': 3264, 'issues': 3265, 'scurvy': 3266, 'talent': 3267, 'dwarf': 3268, 'lefty': 3269, 'minor': 3270, 'moron': 3271, 'pilot': 3272, 'rabbi': 3273, 'average': 3274, 'callous': 3275, 'cynical': 3276, 'dusting': 3277, 'elusive': 3278, 'grouchy': 3279, 'hideous': 3280, 'history': 3281, 'intense': 3282, 'likable': 3283, 'lovable': 3284, 'miserly': 3285, 'idol': 3286, 'neutral': 3287, 'nodding': 3288, 'obscene': 3289, 'pompous': 3290, 'precise': 3291, 'quicker': 3292, 'radical': 3293, 'shaking': 3294, 'shallow': 3295, 'similar': 3296, 'sitting': 3297, 'sloshed': 3298, 'smarter': 3299, 'tactful': 3300, 'unfazed': 3301, 'unmoved': 3302, 'vicious': 3303, 'whining': 3304, 'willing': 3305, 'yawning': 3306, 'sane': 3307, 'knelt': 3308, 'guns': 3309, 'tuna': 3310, 'misled': 3311, 'punched': 3312, 'rewrote': 3313, 'framed': 3314, 'fuming': 3315, 'grumpy': 3316, 'square': 3317, 'touchy': 3318, 'untidy': 3319, 'threat': 3320, 'charm': 3321, 'bury': 3322, 'geniuses': 3323, 'flatter': 3324, 'capsized': 3325, 'instead': 3326, 'balls': 3327, 'chip': 3328, 'boxes': 3329, 'protest': 3330, 'rap': 3331, 'snob': 3332, 'interfere': 3333, 'tool': 3334, 'doughnut': 3335, 'ethiopian': 3336, 'sly': 3337, 'radios': 3338, 'whisky': 3339, 'comedian': 3340, 'southpaw': 3341, 'antisocial': 3342, 'paraplegic': 3343, 'helium': 3344, 'hoist': 3345, 'sails': 3346, 'spain': 3347, 'appreciate': 3348, 'booked': 3349, 'undo': 3350, 'consulted': 3351, 'cycle': 3352, 'feverish': 3353, 'funerals': 3354, 'politics': 3355, 'raccoons': 3356, 'donkey': 3357, 'diabetes': 3358, 'drum': 3359, 'rains': 3360, 'elbow': 3361, 'intend': 3362, 'eggplant': 3363, 'interest': 3364, 'broccoli': 3365, 'comedies': 3366, 'westerns': 3367, 'internet': 3368, 'hammer': 3369, 'bills': 3370, 'biking': 3371, 'recommend': 3372, 'respected': 3373, 'pattern': 3374, 'swedish': 3375, 'supported': 3376, 'entranced': 3377, 'misquoted': 3378, 'soda': 3379, 'jackson': 3380, 'masochist': 3381, 'paramedic': 3382, 'aquarius': 3383, 'catching': 3384, 'chewing': 3385, 'constipated': 3386, 'nearsighted': 3387, 'crook': 3388, 'copilot': 3389, 'waiter': 3390, 'infected': 3391, 'zebra': 3392, 'cafe': 3393, 'ethical': 3394, 'mortifies': 3395, 'ages': 3396, 'enticing': 3397, 'painless': 3398, 'pheasant': 3399, 'dirt': 3400, 'improbable': 3401, 'inadequate': 3402, 'misleading': 3403, 'bomb': 3404, 'updated': 3405, 'knead': 3406, 'dough': 3407, 'lemons': 3408, 'ditch': 3409, 'kites': 3410, 'improvise': 3411, 'alto': 3412, 'shrank': 3413, 'squeak': 3414, 'grey': 3415, 'throat': 3416, 'fence': 3417, 'button': 3418, 'rejection': 3419, 'secure': 3420, 'sharks': 3421, 'caucasian': 3422, 'spanish': 3423, 'spelling': 3424, 'whimpering': 3425, 'pagoda': 3426, 'damp': 3427, 'oven': 3428, 'howled': 3429, 'actors': 3430, 'pilots': 3431, 'tallest': 3432, 'peach': 3433, 'added': 3434, 'mute': 3435, 'cries': 3436, 'disgusts': 3437, 'ducked': 3438, 'exaggerates': 3439, 'sigh': 3440, 'panicky': 3441, 'hunch': 3442, 'options': 3443, 'stamina': 3444, 'opera': 3445, 'barber': 3446, 'bigwig': 3447, 'busker': 3448, 'legend': 3449, 'madman': 3450, 'misfit': 3451, 'pirate': 3452, 'player': 3453, 'potter': 3454, 'rapper': 3455, 'rookie': 3456, 'senior': 3457, 'surfer': 3458, 'tailor': 3459, 'agnostic': 3460, 'adult': 3461, 'credible': 3462, 'cultured': 3463, 'depraved': 3464, 'deranged': 3465, 'diligent': 3466, 'educated': 3467, 'eloquent': 3468, 'emphatic': 3469, 'forceful': 3470, 'giggling': 3471, 'groaning': 3472, 'hammered': 3473, 'humorous': 3474, 'immature': 3475, 'immobile': 3476, 'knocking': 3477, 'learning': 3478, 'likeable': 3479, 'literate': 3480, 'loveable': 3481, 'neurotic': 3482, 'perverse': 3483, 'pleasant': 3484, 'rational': 3485, 'reckless': 3486, 'reformed': 3487, 'resolute': 3488, 'retiring': 3489, 'sickened': 3490, 'sociable': 3491, 'spirited': 3492, 'tactless': 3493, 'terminal': 3494, 'thinking': 3495, 'tireless': 3496, 'underage': 3497, 'unnerved': 3498, 'wavering': 3499, 'wheezing': 3500, 'fazed': 3501, 'nasty': 3502, 'petty': 3503, 'bail': 3504, 'glared': 3505, 'wines': 3506, 'blues': 3507, 'salsa': 3508, 'match': 3509, 'perky': 3510, 'chili': 3511, 'fails': 3512, 'falls': 3513, 'stops': 3514, 'overreacted': 3515, 'rugby': 3516, 'recorded': 3517, 'eager': 3518, 'stoic': 3519, 'sees': 3520, 'tenor': 3521, 'sipped': 3522, 'slowed': 3523, 'rigid': 3524, 'sung': 3525, 'sympathized': 3526, 'abusive': 3527, 'adamant': 3528, 'alerted': 3529, 'blinded': 3530, 'bullied': 3531, 'defiant': 3532, 'elected': 3533, 'evasive': 3534, 'lenient': 3535, 'moaning': 3536, 'tasered': 3537, 'medal': 3538, 'repulsive': 3539, 'yawn': 3540, 'toes': 3541, 'declared': 3542, 'raw': 3543, 'picnics': 3544, 'waffles': 3545, 'showers': 3546, 'chance': 3547, 'fishermen': 3548, 'gardeners': 3549, 'newlyweds': 3550, 'optimists': 3551, 'relatives': 3552, 'soulmates': 3553, 'dude': 3554, 'oar': 3555, 'wolves': 3556, 'sexist': 3557, 'fuse': 3558, 'blown': 3559, 'invisible': 3560, 'dragons': 3561, 'wizard': 3562, 'atlantis': 3563, 'bears': 3564, 'wings': 3565, 'sometime': 3566, 'spell': 3567, 'cherries': 3568, 'classes': 3569, 'supply': 3570, 'hire': 3571, 'tofu': 3572, 'harass': 3573, 'faces': 3574, 'rip': 3575, 'cautiously': 3576, 'protein': 3577, 'gossips': 3578, 'purpose': 3579, 'backward': 3580, 'net': 3581, 'picasso': 3582, 'italian': 3583, 'tenacious': 3584, 'pajamas': 3585, 'sundays': 3586, 'frenchman': 3587, 'nonsmoker': 3588, 'senile': 3589, 'heaven': 3590, 'foreigner': 3591, 'dumbfounded': 3592, 'brazil': 3593, 'norway': 3594, 'frying': 3595, 'witch': 3596, 'bake': 3597, 'collect': 3598, 'lethargic': 3599, 'protected': 3600, 'refreshed': 3601, 'hospitals': 3602, 'grenade': 3603, 'request': 3604, 'scooter': 3605, 'toaster': 3606, 'website': 3607, 'insurance': 3608, 'pneumonia': 3609, 'saturdays': 3610, 'astrology': 3611, 'doughnuts': 3612, 'novel': 3613, 'languages': 3614, 'pop': 3615, 'dinosaurs': 3616, 'envelopes': 3617, 'tape': 3618, 'drums': 3619, 'flute': 3620, 'label': 3621, 'require': 3622, 'rubbed': 3623, 'beside': 3624, 'spent': 3625, 'regularly': 3626, 'divorce': 3627, 'martini': 3628, 'translate': 3629, 'mentor': 3630, 'citizen': 3631, 'groggy': 3632, 'marine': 3633, 'exaggerating': 3634, 'freaking': 3635, 'heading': 3636, 'homeschooled': 3637, 'unprejudiced': 3638, 'servant': 3639, 'reachable': 3640, 'contagious': 3641, 'euros': 3642, 'existed': 3643, 'group': 3644, 'habit': 3645, 'sunflower': 3646, 'illusion': 3647, 'balmy': 3648, 'become': 3649, 'conceivable': 3650, 'fetched': 3651, 'laundry': 3652, 'monkey': 3653, 'trick': 3654, 'sofa': 3655, 'regrettable': 3656, 'transparent': 3657, 'asia': 3658, 'informed': 3659, 'couch': 3660, 'immortal': 3661, 'mars': 3662, 'tomboy': 3663, 'brain': 3664, 'fleas': 3665, 'twitches': 3666, 'numb': 3667, 'goldfish': 3668, 'inbox': 3669, 'beats': 3670, 'neither': 3671, 'stung': 3672, 'reality': 3673, 'rome': 3674, 'sadly': 3675, 'knockout': 3676, 'interfering': 3677, 'superman': 3678, 'pill': 3679, 'explains': 3680, 'enormous': 3681, 'idle': 3682, 'incurable': 3683, 'upsetting': 3684, 'dent': 3685, 'bleated': 3686, 'mic': 3687, 'icy': 3688, 'abated': 3689, 'switch': 3690, 'wound': 3691, 'healed': 3692, 'grave': 3693, 'spotted': 3694, 'thanked': 3695, 'couple': 3696, 'traitors': 3697, 'sabotage': 3698, 'admires': 3699, 'loses': 3700, 'sob': 3701, 'jobs': 3702, 'climbed': 3703, 'concentrated': 3704, 'decorated': 3705, 'explained': 3706, 'wronged': 3707, 'fried': 3708, 'frightens': 3709, 'suntan': 3710, 'tattoo': 3711, 'vision': 3712, 'alibi': 3713, 'butler': 3714, 'ulcer': 3715, 'charisma': 3716, 'chickens': 3717, 'insomnia': 3718, 'leukemia': 3719, 'duis': 3720, 'south': 3721, 'sirens': 3722, 'mark': 3723, 'hoped': 3724, 'fascist': 3725, 'gymnast': 3726, 'liberal': 3727, 'lunatic': 3728, 'realist': 3729, 'recluse': 3730, 'refugee': 3731, 'scumbag': 3732, 'veteran': 3733, 'yodeler': 3734, 'adaptable': 3735, 'intern': 3736, 'beardless': 3737, 'dedicated': 3738, 'defensive': 3739, 'eccentric': 3740, 'effective': 3741, 'elsewhere': 3742, 'energetic': 3743, 'exuberant': 3744, 'fanatical': 3745, 'flustered': 3746, 'foolhardy': 3747, 'honorable': 3748, 'hotheaded': 3749, 'illogical': 3750, 'impassive': 3751, 'lecherous': 3752, 'legendary': 3753, 'muttering': 3754, 'oblivious': 3755, 'observant': 3756, 'observing': 3757, 'obsessive': 3758, 'offensive': 3759, 'guide': 3760, 'paralyzed': 3761, 'possessed': 3762, 'practical': 3763, 'regretful': 3764, 'reluctant': 3765, 'reputable': 3766, 'resentful': 3767, 'resigning': 3768, 'resilient': 3769, 'secretive': 3770, 'shameless': 3771, 'sketching': 3772, 'squatting': 3773, 'surviving': 3774, 'nosey': 3775, 'uncertain': 3776, 'undecided': 3777, 'unethical': 3778, 'unpopular': 3779, 'unrelated': 3780, 'unsettled': 3781, 'weakening': 3782, 'jewish': 3783, 'buying': 3784, 'listed': 3785, 'stable': 3786, 'sighing': 3787, 'ponies': 3788, 'fierce': 3789, 'weaker': 3790, 'overheard': 3791, 'predicted': 3792, 'pressed': 3793, 'pressured': 3794, 'log': 3795, 'silently': 3796, 'shops': 3797, 'shyly': 3798, 'gently': 3799, 'steals': 3800, 'swears': 3801, 'tickled': 3802, 'tires': 3803, 'tormented': 3804, 'appalled': 3805, 'cautious': 3806, 'detained': 3807, 'agony': 3808, 'laid': 3809, 'buddy': 3810, 'pardoned': 3811, 'saddened': 3812, 'selected': 3813, 'unshaven': 3814, 'berserk': 3815, 'wets': 3816, 'paled': 3817, 'hesitating': 3818, 'torture': 3819, 'toss': 3820, 'heat': 3821, 'gunfire': 3822, 'classmates': 3823, 'east': 3824, 'nonsmokers': 3825, 'mates': 3826, 'owners': 3827, 'score': 3828, 'whatever': 3829, 'customs': 3830, 'daddy': 3831, 'notified': 3832, 'banned': 3833, 'wool': 3834, 'dyes': 3835, 'yolks': 3836, 'admission': 3837, 'alcohol': 3838, 'ammonia': 3839, 'base': 3840, 'bedbugs': 3841, 'vampires': 3842, 'vampire': 3843, 'avoid': 3844, 'thieves': 3845, 'build': 3846, 'nests': 3847, 'fascinate': 3848, 'reschedule': 3849, 'bow': 3850, 'champagne': 3851, 'cocaine': 3852, 'compare': 3853, 'disable': 3854, 'ants': 3855, 'sake': 3856, 'latin': 3857, 'threaten': 3858, 'counts': 3859, 'grill': 3860, 'blankets': 3861, 'hamsters': 3862, 'rung': 3863, 'lotus': 3864, 'earns': 3865, 'remorse': 3866, 'dramatist': 3867, 'osaka': 3868, 'honor': 3869, 'imprisoned': 3870, 'timer': 3871, 'joint': 3872, 'raking': 3873, 'trusting': 3874, 'nails': 3875, 'handrail': 3876, 'react': 3877, 'disappointing': 3878, 'mixed': 3879, 'ecuador': 3880, 'argued': 3881, 'bolted': 3882, 'cactus': 3883, 'hybrid': 3884, 'braked': 3885, 'chopin': 3886, 'graduate': 3887, 'scarf': 3888, 'grade': 3889, 'lightly': 3890, 'desert': 3891, 'proposal': 3892, 'sailboat': 3893, 'solution': 3894, 'opinion': 3895, 'buck': 3896, 'discs': 3897, 'regrets': 3898, 'sunglasses': 3899, 'nieces': 3900, 'interrupted': 3901, 'navy': 3902, 'confidence': 3903, 'adventures': 3904, 'challenges': 3905, 'folk': 3906, 'sandwiches': 3907, 'skirt': 3908, 'hats': 3909, 'model': 3910, 'film': 3911, 'hamburgers': 3912, 'spicy': 3913, 'minutes': 3914, 'calendar': 3915, 'keyboard': 3916, 'toothpaste': 3917, 'muscle': 3918, 'fuel': 3919, 'hill': 3920, 'reached': 3921, 'sunrise': 3922, 'waterbed': 3923, 'plates': 3924, 'outsider': 3925, 'discouraged': 3926, 'overwhelmed': 3927, 'flirting': 3928, 'lottery': 3929, 'dentist': 3930, 'grandfather': 3931, 'hairdresser': 3932, 'astronomer': 3933, 'concentrating': 3934, 'conscientious': 3935, 'hoping': 3936, 'maid': 3937, 'fashioned': 3938, 'uncomfortable': 3939, 'discrete': 3940, 'evicted': 3941, 'pieces': 3942, 'ways': 3943, 'cooled': 3944, 'egyptian': 3945, 'chaos': 3946, 'confuse': 3947, 'th': 3948, 'dictionary': 3949, 'rage': 3950, 'confidential': 3951, 'rumor': 3952, 'issue': 3953, 'pleasure': 3954, 'unacceptable': 3955, 'bends': 3956, 'ruin': 3957, 'enjoyable': 3958, 'midwife': 3959, 'vivacious': 3960, 'vibrated': 3961, 'gambling': 3962, 'visits': 3963, 'persuaded': 3964, 'opposites': 3965, 'attract': 3966, 'poets': 3967, 'apron': 3968, 'shawl': 3969, 'dear': 3970, 'polish': 3971, 'silence': 3972, 'golden': 3973, 'silk': 3974, 'sloths': 3975, 'solve': 3976, 'somehow': 3977, 'badgering': 3978, 'interrupting': 3979, 'termites': 3980, 'blow': 3981, 'super': 3982, 'spirit': 3983, 'handy': 3984, 'belt': 3985, 'sped': 3986, 'bounced': 3987, 'squeaked': 3988, 'ajar': 3989, 'doorbell': 3990, 'rotates': 3991, 'lemon': 3992, 'lovers': 3993, 'market': 3994, 'meager': 3995, 'sail': 3996, 'verdict': 3997, 'gods': 3998, 'menaced': 3999, 'hunted': 4000, 'foxes': 4001, 'cannibals': 4002, 'identical': 4003, 'murderers': 4004, 'coconut': 4005, 'adverb': 4006, 'priceless': 4007, 'silver': 4008, 'thursday': 4009, 'aced': 4010, 'arrives': 4011, 'bikes': 4012, 'claims': 4013, 'demanded': 4014, 'beggar': 4015, 'pauper': 4016, 'donated': 4017, 'suv': 4018, 'dyed': 4019, 'excluded': 4020, 'nauseous': 4021, 'evaluated': 4022, 'rhubarb': 4023, 'seizure': 4024, 'iq': 4025, 'rowboat': 4026, 'allergies': 4027, 'iphone': 4028, 'arthritis': 4029, 'awol': 4030, 'arms': 4031, 'potential': 4032, 'imitated': 4033, 'jeweller': 4034, 'mechanic': 4035, 'minister': 4036, 'pushover': 4037, 'reporter': 4038, 'stranger': 4039, 'telepath': 4040, 'virtuoso': 4041, 'amateur': 4042, 'analytical': 4043, 'articulate': 4044, 'believable': 4045, 'colorblind': 4046, 'eyed': 4047, 'dependable': 4048, 'determined': 4049, 'diplomatic': 4050, 'displeased': 4051, 'exercising': 4052, 'expendable': 4053, 'gregarious': 4054, 'handcuffed': 4055, 'indecisive': 4056, 'inflexible': 4057, 'infuriated': 4058, 'intolerant': 4059, 'mesmerized': 4060, 'methodical': 4061, 'motionless': 4062, 'destiny': 4063, 'stepdad': 4064, 'stepson': 4065, 'mysterious': 4066, 'particular': 4067, 'passionate': 4068, 'persistent': 4069, 'personable': 4070, 'perspiring': 4071, 'persuasive': 4072, 'productive': 4073, 'recovering': 4074, 'relentless': 4075, 'remodeling': 4076, 'remorseful': 4077, 'respectful': 4078, 'struggling': 4079, 'stuttering': 4080, 'suspicious': 4081, 'killer': 4082, 'winner': 4083, 'thoughtful': 4084, 'unaffected': 4085, 'unbeatable': 4086, 'undeterred': 4087, 'unfaithful': 4088, 'unmerciful': 4089, 'unpleasant': 4090, 'unprepared': 4091, 'pious': 4092, 'vulnerable': 4093, 'budging': 4094, 'scrawny': 4095, 'trained': 4096, 'cleaning': 4097, 'crawling': 4098, 'empathy': 4099, 'lent': 4100, 'lifts': 4101, 'weights': 4102, 'camping': 4103, 'farming': 4104, 'lobster': 4105, 'sausage': 4106, 'bowl': 4107, 'bashful': 4108, 'foreign': 4109, 'haggard': 4110, 'rattled': 4111, 'thinner': 4112, 'risotto': 4113, 'rituals': 4114, 'pancakes': 4115, 'peeked': 4116, 'peered': 4117, 'cricket': 4118, 'reacted': 4119, 'received': 4120, 'patiently': 4121, 'sensed': 4122, 'squatted': 4123, 'stays': 4124, 'stiffly': 4125, 'bait': 4126, 'travels': 4127, 'vowed': 4128, 'results': 4129, 'updates': 4130, 'assaulted': 4131, 'astounded': 4132, 'carjacked': 4133, 'chuckling': 4134, 'misery': 4135, 'mystified': 4136, 'strangled': 4137, 'clubbing': 4138, 'fedora': 4139, 'tuxedo': 4140, 'indoors': 4141, 'channel': 4142, 'hibernate': 4143, 'beers': 4144, 'acrylic': 4145, 'venus': 4146, 'liquid': 4147, 'suppliers': 4148, 'parks': 4149, 'victory': 4150, 'donations': 4151, 'canoe': 4152, 'seek': 4153, 'happiness': 4154, 'related': 4155, 'adults': 4156, 'mourning': 4157, 'journalists': 4158, 'killers': 4159, 'workaholics': 4160, 'speeding': 4161, 'stain': 4162, 'major': 4163, 'occur': 4164, 'brandy': 4165, 'invented': 4166, 'mona': 4167, 'lisa': 4168, 'flammable': 4169, 'actresses': 4170, 'stunning': 4171, 'spot': 4172, 'safer': 4173, 'telling': 4174, 'scratched': 4175, 'revolt': 4176, 'believer': 4177, 'blaming': 4178, 'contains': 4179, 'hops': 4180, 'chirping': 4181, 'changing': 4182, 'pillow': 4183, 'rackets': 4184, 'subject': 4185, 'pockets': 4186, 'bribe': 4187, 'stitches': 4188, 'gums': 4189, 'bleed': 4190, 'paranoid': 4191, 'grim': 4192, 'scene': 4193, 'pressure': 4194, 'relaxes': 4195, 'dollar': 4196, 'desire': 4197, 'remote': 4198, 'thanksgiving': 4199, 'foolishly': 4200, 'advised': 4201, 'caution': 4202, 'mile': 4203, 'cartwheel': 4204, 'cycling': 4205, 'gripped': 4206, 'alzheimer': 4207, 'footsteps': 4208, 'held': 4209, 'mere': 4210, 'agent': 4211, 'witted': 4212, 'judgement': 4213, 'luxury': 4214, 'temper': 4215, 'apology': 4216, 'posed': 4217, 'thrust': 4218, 'affection': 4219, 'ipad': 4220, 'weighs': 4221, 'kilos': 4222, 'creationist': 4223, 'ghostwriter': 4224, 'teetotaller': 4225, 'englishman': 4226, 'self': 4227, 'employed': 4228, 'torn': 4229, 'sooty': 4230, 'smooth': 4231, 'baggage': 4232, 'doom': 4233, 'eludes': 4234, 'pace': 4235, 'quickened': 4236, 'advise': 4237, 'customers': 4238, 'completely': 4239, 'boiling': 4240, 'grilling': 4241, 'prophet': 4242, 'anticipated': 4243, 'discuss': 4244, 'sin': 4245, 'code': 4246, 'deleted': 4247, 'willingly': 4248, 'dreamt': 4249, 'entered': 4250, 'cave': 4251, 'miserably': 4252, 'feared': 4253, 'fractured': 4254, 'stage': 4255, 'fright': 4256, 'loads': 4257, 'eyebrows': 4258, 'siblings': 4259, 'rehearse': 4260, 'explosions': 4261, 'hid': 4262, 'jog': 4263, 'imagination': 4264, 'l': 4265, 'races': 4266, 'raspberries': 4267, 'tomato': 4268, 'poverty': 4269, 'cider': 4270, 'bearded': 4271, 'butterflies': 4272, 'fairy': 4273, 'tales': 4274, 'offense': 4275, 'cigarette': 4276, 'stopwatch': 4277, 'volunteer': 4278, 'body': 4279, 'information': 4280, 'inspiration': 4281, 'pocketed': 4282, 'ideas': 4283, 'pictures': 4284, 'sort': 4285, 'psychology': 4286, 'value': 4287, 'attorney': 4288, 'details': 4289, 'interrogated': 4290, 'television': 4291, 'coerced': 4292, 'accompany': 4293, 'fiancee': 4294, 'guardian': 4295, 'dweller': 4296, 'swimmer': 4297, 'guesser': 4298, 'lousy': 4299, 'photographer': 4300, 'screenwriter': 4301, 'library': 4302, 'claustrophobic': 4303, 'singapore': 4304, 'bathtub': 4305, 'beginning': 4306, 'sock': 4307, 'magician': 4308, 'denying': 4309, 'slave': 4310, 'balcony': 4311, 'shape': 4312, 'terminally': 4313, 'compliment': 4314, 'fixing': 4315, 'chained': 4316, 'milkman': 4317, 'repaired': 4318, 'parallel': 4319, 'handcrafted': 4320, 'preventable': 4321, 'unavoidable': 4322, 'profile': 4323, 'cuban': 4324, 'cigar': 4325, 'policy': 4326, 'proven': 4327, 'herring': 4328, 'corner': 4329, 'merely': 4330, 'impersonal': 4331, 'crowded': 4332, 'totally': 4333, 'alarming': 4334, 'greece': 4335, 'knock': 4336, 'laws': 4337, 'lesson': 4338, 'iced': 4339, 'waitress': 4340, 'soprano': 4341, 'nutritious': 4342, 'toyota': 4343, 'racing': 4344, 'thinks': 4345, 'la': 4346, 'forever': 4347, 'official': 4348, 'restored': 4349, 'bamboo': 4350, 'trash': 4351, 'bandage': 4352, 'immediately': 4353, 'strength': 4354, 'sorceress': 4355, 'twenty': 4356, 'leaped': 4357, 'joy': 4358, 'lonesome': 4359, 'antiques': 4360, 'happily': 4361, 'alcoholic': 4362, 'losers': 4363, 'intriguing': 4364, 'suckers': 4365, 'juvenile': 4366, 'trigger': 4367, 'unfortunate': 4368, 'unimportant': 4369, 'attempt': 4370, 'axle': 4371, 'packed': 4372, 'noisy': 4373, 'stale': 4374, 'buzzer': 4375, 'concert': 4376, 'doorknob': 4377, 'flame': 4378, 'fog': 4379, 'lifted': 4380, 'gossiped': 4381, 'melted': 4382, 'lights': 4383, 'squealed': 4384, 'ocean': 4385, 'pan': 4386, 'parrot': 4387, 'pond': 4388, 'printer': 4389, 'server': 4390, 'shirts': 4391, 'signal': 4392, 'spider': 4393, 'setting': 4394, 'shining': 4395, 'waves': 4396, 'rotten': 4397, 'wrestlers': 4398, 'crept': 4399, 'families': 4400, 'carnations': 4401, 'everywhere': 4402, 'foreigners': 4403, 'shaggy': 4404, 'disturbing': 4405, 'kitchen': 4406, 'plagiarism': 4407, 'crude': 4408, 'soil': 4409, 'moist': 4410, 'tacos': 4411, 'clash': 4412, 'blush': 4413, 'condo': 4414, 'pork': 4415, 'trout': 4416, 'claimed': 4417, 'comforted': 4418, 'cycles': 4419, 'combat': 4420, 'typhus': 4421, 'lemonade': 4422, 'backwards': 4423, 'fled': 4424, 'canada': 4425, 'follows': 4426, 'bravely': 4427, 'ponytail': 4428, 'potbelly': 4429, 'tricycle': 4430, 'curly': 4431, 'savings': 4432, 'calculus': 4433, 'target': 4434, 'capricorn': 4435, 'bartender': 4436, 'eater': 4437, 'biologist': 4438, 'dense': 4439, 'librarian': 4440, 'nerd': 4441, 'scoundrel': 4442, 'socialist': 4443, 'sophomore': 4444, 'terrorist': 4445, 'zookeeper': 4446, 'employee': 4447, 'considerate': 4448, 'nypd': 4449, 'gravely': 4450, 'happier': 4451, 'yard': 4452, 'insensitive': 4453, 'intoxicated': 4454, 'pessimistic': 4455, 'stalking': 4456, 'thickheaded': 4457, 'throwing': 4458, 'unstoppable': 4459, 'choosy': 4460, 'collapsed': 4461, 'lacrosse': 4462, 'macaroni': 4463, 'redheads': 4464, 'limps': 4465, 'slightly': 4466, 'pensive': 4467, 'partying': 4468, 'milked': 4469, 'cow': 4470, 'theater': 4471, 'raises': 4472, 'orchids': 4473, 'reassured': 4474, 'salted': 4475, 'scrunched': 4476, 'amiably': 4477, 'rowing': 4478, 'scarlet': 4479, 'wakes': 4480, 'trade': 4481, 'dumbstruck': 4482, 'salivating': 4483, 'contacts': 4484, 'force': 4485, 'buzzed': 4486, 'natural': 4487, 'uranus': 4488, 'instincts': 4489, 'affects': 4490, 'basically': 4491, 'chartered': 4492, 'chatted': 4493, 'wheat': 4494, 'ample': 4495, 'gunshot': 4496, 'passengers': 4497, 'conservative': 4498, 'enthusiastic': 4499, 'honeymooning': 4500, 'ignoring': 4501, 'jacksons': 4502, 'cia': 4503, 'whales': 4504, 'mammals': 4505, 'pumpkin': 4506, 'coincidence': 4507, 'fellow': 4508, 'hemoglobin': 4509, 'station': 4510, 'porcupine': 4511, 'bugging': 4512, 'funeral': 4513, 'wedding': 4514, 'hidden': 4515, 'uniform': 4516, 'bakery': 4517, 'museum': 4518, 'toilet': 4519, 'prevent': 4520, 'volunteering': 4521, 'float': 4522, 'chosen': 4523, 'oath': 4524, 'cowards': 4525, 'racists': 4526, 'princess': 4527, 'predictable': 4528, 'four': 4529, 'whale': 4530, 'mammal': 4531, 'sales': 4532, 'aluminum': 4533, 'imagining': 4534, 'swans': 4535, 'seats': 4536, 'rumors': 4537, 'survivors': 4538, 'independent': 4539, 'least': 4540, 'ducks': 4541, 'overrated': 4542, 'swimsuit': 4543, 'blanket': 4544, 'copies': 4545, 'credit': 4546, 'cherish': 4547, 'chivalry': 4548, 'measure': 4549, 'o': 4550, 'competition': 4551, 'virus': 4552, 'elaborate': 4553, 'cyprus': 4554, 'island': 4555, 'purchase': 4556, 'divide': 4557, 'b': 4558}\n"
     ]
    }
   ],
   "source": [
    "print(inp_lang.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 848,
     "status": "ok",
     "timestamp": 1581240543617,
     "user": {
      "displayName": "نیره سادات خلدی نسب",
      "photoUrl": "",
      "userId": "03153259151600659487"
     },
     "user_tz": -210
    },
    "id": "qc6-NK1GtWQt",
    "outputId": "ec4b1d27-b429-4a63-b431-f9dc1f5bdd17"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 10]), TensorShape([64, 14]))"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ggi2nyxOWhhn"
   },
   "source": [
    "#Part A (Embedding  or One-hot ):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sm7NH05Rwbpe"
   },
   "source": [
    "###Encoder With Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nZ2rI24i3jFg"
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.training.tracking.data_structures import NoDependency\n",
    "from tensorflow.python.framework.tensor_shape import TensorShape\n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.enc_units = enc_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.lstm = tf.keras.layers.LSTM(self.enc_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='orthogonal')\n",
    "    \n",
    "  def call(self, x):\n",
    "    x = self.embedding(x)\n",
    "    print('Embed : ',tf.shape(x))\n",
    "    output, state, _ = self.lstm(x, initial_state = None)\n",
    "    return output, state\n",
    "\n",
    "  def initialize_hidden_state(self):\n",
    "    return tf.zeros((self.batch_sz, self.enc_units))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4171,
     "status": "ok",
     "timestamp": 1581241470441,
     "user": {
      "displayName": "نیره سادات خلدی نسب",
      "photoUrl": "",
      "userId": "03153259151600659487"
     },
     "user_tz": -210
    },
    "id": "60gSVh05Jl6l",
    "outputId": "f6a23d0f-4712-4027-e8b4-8087b49122bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embed :  tf.Tensor([ 64  10 256], shape=(3,), dtype=int32)\n",
      "Encoder output shape: (batch size, sequence length, units) (64, 10, 1024)\n",
      "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "sample_output, sample_hidden = encoder(example_input_batch)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yJ_B3mhW3jFk"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.dec_units = dec_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.lstm = tf.keras.layers.LSTM(self.dec_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "  def call(self, x, hidden):\n",
    "    \n",
    "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "    x = self.embedding(x)\n",
    "\n",
    "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "    x = tf.concat([tf.expand_dims(hidden, 1), x], axis=-1)\n",
    "\n",
    "    # passing the concatenated vector to the GRU\n",
    "    output, state, _ = self.lstm(x)\n",
    "\n",
    "    # output shape == (batch_size * 1, hidden_size)\n",
    "    output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "    # output shape == (batch_size, vocab)\n",
    "    x = self.fc(output)\n",
    "\n",
    "    return x, state#, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1811,
     "status": "ok",
     "timestamp": 1581241474159,
     "user": {
      "displayName": "نیره سادات خلدی نسب",
      "photoUrl": "",
      "userId": "03153259151600659487"
     },
     "user_tz": -210
    },
    "id": "P5UY8wko3jFp",
    "outputId": "ac79c288-1bf9-40c4-d529-0840a3f703b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (64, 7263)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "sample_decoder_output, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                      sample_hidden)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_ch_71VbIRfK"
   },
   "source": [
    "## Define the optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WmTHr5iV3jFr"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DMVWzzsfNl4e"
   },
   "source": [
    "## Checkpoints (Object-based saving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zj8bXQTgNwrF"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints/embedding'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hpObfY22IddU"
   },
   "source": [
    "## Training\n",
    "\n",
    "1. Pass the *input* through the *encoder* which return *encoder output* and the *encoder hidden state*.\n",
    "2. The encoder output, encoder hidden state and the decoder input (which is the *start token*) is passed to the decoder.\n",
    "3. The decoder returns the *predictions* and the *decoder hidden state*.\n",
    "4. The decoder hidden state is then passed back into the model and the predictions are used to calculate the loss.\n",
    "5. Use *teacher forcing* to decide the next input to the decoder.\n",
    "6. *Teacher forcing* is the technique where the *target word* is passed as the *next input* to the decoder.\n",
    "7. The final step is to calculate the gradients and apply it to the optimizer and backpropagate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sC9ArXSsVfqn"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "  loss = 0\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "    enc_output, enc_hidden = encoder(inp)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "    # Teacher forcing - feeding the target as the next input\n",
    "    print(targ.shape[1])\n",
    "    for t in range(1, targ.shape[1]):\n",
    "      # passing enc_output to the decoder\n",
    "      predictions, dec_hidden = decoder(dec_input, dec_hidden)\n",
    "\n",
    "      loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "      # using teacher forcing\n",
    "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "  batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "  print('gradients')\n",
    "  gradients = tape.gradient(loss, variables)\n",
    "\n",
    "  print('optimizer')\n",
    "  optimizer.apply_gradients(zip(gradients, variables))\n",
    "  print('batch_loss')\n",
    "  return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1336234,
     "status": "ok",
     "timestamp": 1581242839069,
     "user": {
      "displayName": "نیره سادات خلدی نسب",
      "photoUrl": "",
      "userId": "03153259151600659487"
     },
     "user_tz": -210
    },
    "id": "ddefjBMa3jF0",
    "outputId": "5b3fa747-a2d4-42ba-911a-89c286867792"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embed :  Tensor(\"encoder_3/Shape:0\", shape=(3,), dtype=int32)\n",
      "14\n",
      "gradients\n",
      "optimizer\n",
      "batch_loss\n",
      "Embed :  Tensor(\"encoder_3/Shape:0\", shape=(3,), dtype=int32)\n",
      "14\n",
      "gradients\n",
      "optimizer\n",
      "batch_loss\n",
      "0\n",
      "Epoch 1 Batch 0 Loss 3.7408\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "Epoch 1 Batch 100 Loss 1.7035\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "Epoch 1 Batch 200 Loss 1.5275\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "Epoch 1 Batch 300 Loss 1.5194\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "Epoch 1 Loss 1.7292\n",
      "Time taken for 1 epoch 1333.401370048523 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 1\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "\n",
    "  enc_hidden = encoder.initialize_hidden_state()\n",
    "  total_loss = 0\n",
    "\n",
    "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "    batch_loss = train_step(inp, targ, enc_hidden)\n",
    "    total_loss += batch_loss\n",
    "    print(batch)\n",
    "\n",
    "    if batch % 100 == 0:\n",
    "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                   batch,\n",
    "                                                   batch_loss.numpy()))\n",
    "  # saving (checkpoint) the model every 2 epochs\n",
    "  if (epoch + 1) % 2 == 0:\n",
    "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mU3Ce8M6I3rz"
   },
   "source": [
    "## Translate\n",
    "\n",
    "* The evaluate function is similar to the training loop, except we don't use *teacher forcing* here. The input to the decoder at each time step is its previous predictions along with the hidden state and the encoder output.\n",
    "* Stop predicting when the model predicts the *end token*.\n",
    "* And store the *attention weights for every time step*.\n",
    "\n",
    "Note: The encoder output is calculated only once for one input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EbQpyYs13jF_"
   },
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "  attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "\n",
    "  sentence = preprocess_sentence(sentence)\n",
    "\n",
    "  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                         maxlen=max_length_inp,\n",
    "                                                         padding='post')\n",
    "  inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "  result = ''\n",
    "\n",
    "  hidden = [tf.zeros((1, units))]\n",
    "  enc_out, enc_hidden = encoder(inputs)\n",
    "\n",
    "  dec_hidden = enc_hidden\n",
    "  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
    "\n",
    "  for t in range(max_length_targ):\n",
    "    predictions, dec_hidden = decoder(dec_input, dec_hidden)\n",
    "\n",
    "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "    result += targ_lang.index_word[predicted_id] + ' '\n",
    "\n",
    "    if targ_lang.index_word[predicted_id] == '<end>':\n",
    "      return result, sentence\n",
    "\n",
    "    # the predicted ID is fed back into the model\n",
    "    dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "  return result, sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sl9zUHzg3jGI"
   },
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "  result, sentence = evaluate(sentence)\n",
    "\n",
    "  print('Input: %s' % (sentence))\n",
    "  print('Predicted translation: {}'.format(result))\n",
    "\n",
    "  #attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "  #plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n250XbnjOaqP"
   },
   "source": [
    "## Restore the latest checkpoint and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 963,
     "status": "ok",
     "timestamp": 1581243250069,
     "user": {
      "displayName": "نیره سادات خلدی نسب",
      "photoUrl": "",
      "userId": "03153259151600659487"
     },
     "user_tz": -210
    },
    "id": "UJpT9D5_OgP6",
    "outputId": "aced03b9-74da-4964-aecb-a5eca49983e6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.InitializationOnlyStatus at 0x7fbf55615c88>"
      ]
     },
     "execution_count": 68,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint_dir = './training_checkpoints/embedding'\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 983,
     "status": "ok",
     "timestamp": 1581243252320,
     "user": {
      "displayName": "نیره سادات خلدی نسب",
      "photoUrl": "",
      "userId": "03153259151600659487"
     },
     "user_tz": -210
    },
    "id": "NgzuRB7WbayJ",
    "outputId": "70eac660-0df5-4956-b8f8-a55e1b8b980a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<start> hi . <end>', '<start> hi . <end>', '<start> run ! <end>', '<start> wow ! <end>')\n",
      "('<start> hallo ! <end>', '<start> gru gott ! <end>', '<start> lauf ! <end>', '<start> potzdonner ! <end>')\n"
     ]
    }
   ],
   "source": [
    "print(English[0:4])\n",
    "print(German[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1656,
     "status": "ok",
     "timestamp": 1581244455517,
     "user": {
      "displayName": "نیره سادات خلدی نسب",
      "photoUrl": "",
      "userId": "03153259151600659487"
     },
     "user_tz": -210
    },
    "id": "WrAM0FDomq3E",
    "outputId": "1a63b01d-ea7e-4d48-a4c3-4596373ed4b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embed :  tf.Tensor([  1  10 256], shape=(3,), dtype=int32)\n",
      "Input: <start> hi . <end>\n",
      "Predicted translation: ich bin ein guter . <end> \n"
     ]
    }
   ],
   "source": [
    "translate(u' hi . ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 912,
     "status": "ok",
     "timestamp": 1581243317350,
     "user": {
      "displayName": "نیره سادات خلدی نسب",
      "photoUrl": "",
      "userId": "03153259151600659487"
     },
     "user_tz": -210
    },
    "id": "zSx2iM36EZQZ",
    "outputId": "30e1653a-3e5b-4836-f4bb-496f875e0906"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embed :  tf.Tensor([  1  10 256], shape=(3,), dtype=int32)\n",
      "Input: <start> wow ! <end>\n",
      "Predicted translation: ich bin ein guter . <end> \n"
     ]
    }
   ],
   "source": [
    "translate(u'wow !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A3LLCx3ZE0Ls"
   },
   "outputs": [],
   "source": [
    "translate(u'¿todavia estan en casa?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DUQVLVqUE1YW"
   },
   "outputs": [],
   "source": [
    "# wrong translation\n",
    "translate(u'trata de averiguarlo.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RTe5P5ioMJwN"
   },
   "source": [
    "## Next steps\n",
    "\n",
    "* [Download a different dataset](http://www.manythings.org/anki/) to experiment with translations, for example, English to German, or English to French.\n",
    "* Experiment with training on a larger dataset, or using more epochs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6vc_yxxIw0f6"
   },
   "source": [
    "##Encoder Without Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8htKMQziw0gh"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.enc_units = enc_units\n",
    "    #self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.lstm = tf.keras.layers.LSTM(self.enc_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "\n",
    "  def call(self, x):\n",
    "    #print('x : ',tf.keras.utils.to_categorical(x,vocab_inp_size))\n",
    "    x = tf.keras.utils.to_categorical(x,vocab_inp_size)\n",
    "    #x = self.embedding(x)\n",
    "    #print('Embed : ',tf.shape(x))\n",
    "    output, state, _ = self.lstm(x, initial_state = None)\n",
    "    return output, state\n",
    "    \n",
    "  def initialize_hidden_state(self):\n",
    "    return tf.zeros((self.batch_sz, self.enc_units))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3581,
     "status": "ok",
     "timestamp": 1581241178678,
     "user": {
      "displayName": "نیره سادات خلدی نسب",
      "photoUrl": "",
      "userId": "03153259151600659487"
     },
     "user_tz": -210
    },
    "id": "jnyO0QNew0g0",
    "outputId": "026f3c1c-6997-44c8-a327-31f15ffde232"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (64, 10, 1024)\n",
      "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "# sample input\n",
    "sample_output, sample_hidden = encoder(example_input_batch)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IRrUI8Hkw0hJ"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.dec_units = dec_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.lstm = tf.keras.layers.LSTM(self.dec_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "  def call(self, x, hidden):\n",
    "    \n",
    "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "    x = self.embedding(x)\n",
    "\n",
    "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "    x = tf.concat([tf.expand_dims(hidden, 1), x], axis=-1)\n",
    "\n",
    "    # passing the concatenated vector to the GRU\n",
    "    output, state, _ = self.lstm(x)\n",
    "\n",
    "    # output shape == (batch_size * 1, hidden_size)\n",
    "    output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "    # output shape == (batch_size, vocab)\n",
    "    x = self.fc(output)\n",
    "\n",
    "    return x, state#, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2104,
     "status": "ok",
     "timestamp": 1581241183344,
     "user": {
      "displayName": "نیره سادات خلدی نسب",
      "photoUrl": "",
      "userId": "03153259151600659487"
     },
     "user_tz": -210
    },
    "id": "OeclO-4Fw0hZ",
    "outputId": "d0474fa7-c5d9-4fff-b126-06f8838f8aa6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (64, 7263)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "sample_decoder_output, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                      sample_hidden)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y9uw8M0Ew0hl"
   },
   "source": [
    "## Define the optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U4-x18Ocw0hs"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cwJPEox4w0h2"
   },
   "source": [
    "## Checkpoints (Object-based saving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AN8VA0Bmw0h6"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints/one_hot'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CUXQon0Vw0iE"
   },
   "source": [
    "## Training\n",
    "\n",
    "1. Pass the *input* through the *encoder* which return *encoder output* and the *encoder hidden state*.\n",
    "2. The encoder output, encoder hidden state and the decoder input (which is the *start token*) is passed to the decoder.\n",
    "3. The decoder returns the *predictions* and the *decoder hidden state*.\n",
    "4. The decoder hidden state is then passed back into the model and the predictions are used to calculate the loss.\n",
    "5. Use *teacher forcing* to decide the next input to the decoder.\n",
    "6. *Teacher forcing* is the technique where the *target word* is passed as the *next input* to the decoder.\n",
    "7. The final step is to calculate the gradients and apply it to the optimizer and backpropagate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q7Cj6xq_w0iH"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "  loss = 0\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "    enc_output, enc_hidden = encoder(inp)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "    # Teacher forcing - feeding the target as the next input\n",
    "    print(targ.shape[1])\n",
    "    for t in range(1, targ.shape[1]):\n",
    "      # passing enc_output to the decoder\n",
    "      predictions, dec_hidden = decoder(dec_input, dec_hidden)\n",
    "\n",
    "      loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "      # using teacher forcing\n",
    "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "  batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "  print('gradients')\n",
    "  gradients = tape.gradient(loss, variables)\n",
    "\n",
    "  print('optimizer')\n",
    "  optimizer.apply_gradients(zip(gradients, variables))\n",
    "  print('batch_loss')\n",
    "  return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1801,
     "status": "error",
     "timestamp": 1581241217680,
     "user": {
      "displayName": "نیره سادات خلدی نسب",
      "photoUrl": "",
      "userId": "03153259151600659487"
     },
     "user_tz": -210
    },
    "id": "HnmA7oLHw0iU",
    "outputId": "fadaab82-2811-4bcc-c19a-f95925e1031b"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-8028073afa88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    604\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2360\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2362\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2363\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2702\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2703\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2704\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2705\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2591\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2592\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2593\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   2594\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2595\u001b[0m         \u001b[0;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    976\u001b[0m                                           converted_func)\n\u001b[1;32m    977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 978\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: in converted code:\n\n    <ipython-input-36-7ecda3e511ca>:6 train_step  *\n        enc_output, enc_hidden = encoder(inp)\n    /tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/base_layer.py:778 __call__\n        outputs = call_fn(cast_inputs, *args, **kwargs)\n    <ipython-input-21-97b395797213>:16 call  *\n        x = tf.keras.utils.to_categorical(x,vocab_inp_size)\n    /tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/utils/np_utils.py:40 to_categorical\n        y = np.array(y, dtype='int')\n\n    TypeError: __array__() takes 1 positional argument but 2 were given\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 1\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "\n",
    "  enc_hidden = encoder.initialize_hidden_state()\n",
    "  total_loss = 0\n",
    "\n",
    "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "    batch_loss = train_step(inp, targ, enc_hidden)\n",
    "    total_loss += batch_loss\n",
    "    print(batch)\n",
    "\n",
    "    if batch % 100 == 0:\n",
    "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                   batch,\n",
    "                                                   batch_loss.numpy()))\n",
    "  # saving (checkpoint) the model every 2 epochs\n",
    "  if (epoch + 1) % 2 == 0:\n",
    "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4fpbJZedw0ie"
   },
   "source": [
    "## Translate\n",
    "\n",
    "* The evaluate function is similar to the training loop, except we don't use *teacher forcing* here. The input to the decoder at each time step is its previous predictions along with the hidden state and the encoder output.\n",
    "* Stop predicting when the model predicts the *end token*.\n",
    "* And store the *attention weights for every time step*.\n",
    "\n",
    "Note: The encoder output is calculated only once for one input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uhdiS-qiw0ih"
   },
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "  attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "\n",
    "  sentence = preprocess_sentence(sentence)\n",
    "\n",
    "  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                         maxlen=max_length_inp,\n",
    "                                                         padding='post')\n",
    "  inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "  result = ''\n",
    "\n",
    "  hidden = [tf.zeros((1, units))]\n",
    "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "  dec_hidden = enc_hidden\n",
    "  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
    "\n",
    "  for t in range(max_length_targ):\n",
    "    predictions, dec_hidden = decoder(dec_input, dec_hidden)\n",
    "\n",
    "    # storing the attention weights to plot later on\n",
    "    #attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "    #attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "    result += targ_lang.index_word[predicted_id] + ' '\n",
    "\n",
    "    if targ_lang.index_word[predicted_id] == '<end>':\n",
    "      return result, sentence\n",
    "\n",
    "    # the predicted ID is fed back into the model\n",
    "    dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "  return result, sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IwIQniI3w0ir"
   },
   "outputs": [],
   "source": [
    "# function for plotting the attention weights\n",
    "'''\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "  fig = plt.figure(figsize=(10,10))\n",
    "  ax = fig.add_subplot(1, 1, 1)\n",
    "  ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "  fontdict = {'fontsize': 14}\n",
    "\n",
    "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "  plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_mLcT_hfw0i4"
   },
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "  result, sentence = evaluate(sentence)\n",
    "\n",
    "  print('Input: %s' % (sentence))\n",
    "  print('Predicted translation: {}'.format(result))\n",
    "\n",
    "  #attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "  #plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oLzGwS4uw0jD"
   },
   "source": [
    "## Restore the latest checkpoint and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ghOlNxujw0jH"
   },
   "outputs": [],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZOt8PMaFw0jU"
   },
   "outputs": [],
   "source": [
    "translate(u'hace mucho frio aqui.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YNlO6KI5w0jm"
   },
   "outputs": [],
   "source": [
    "translate(u'esta es mi vida.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0zUlMA8bw0j0"
   },
   "outputs": [],
   "source": [
    "translate(u'¿todavia estan en casa?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rjoaYMi2w0kE"
   },
   "outputs": [],
   "source": [
    "# wrong translation\n",
    "translate(u'trata de averiguarlo.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DTEFcxgpw0kP"
   },
   "source": [
    "## Next steps\n",
    "\n",
    "* [Download a different dataset](http://www.manythings.org/anki/) to experiment with translations, for example, English to German, or English to French.\n",
    "* Experiment with training on a larger dataset, or using more epochs\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNv1k8WysF8HxoWKmUibDUP",
   "collapsed_sections": [],
   "name": "Final_Machine translation.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
